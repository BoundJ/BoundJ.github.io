<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="为什么要使用protobuf？一个好的软件框架应该要有明确的输入和输出，对于CNN网络而言，其主要有两部分组成：网络具体结构和网络的具体优化算法及参数。对于框架的使用者而言，用户只需输入两个描述文件即可得到对该网络的优化结果，这无疑是非常方便的。
所以说，这是为了方便配置和使用caffe。

caffe框架选择使用谷歌的开源protobuf工具对这两部分进行描述，解析和存储，这一部分为caffe的">
<meta property="og:type" content="article">
<meta property="og:title" content="Protobuf and caffe.proto">
<meta property="og:url" content="http://verdin.cn/2017/03/10/Protobuf-and-caffe-proto/index.html">
<meta property="og:site_name" content="Verdin小站">
<meta property="og:description" content="为什么要使用protobuf？一个好的软件框架应该要有明确的输入和输出，对于CNN网络而言，其主要有两部分组成：网络具体结构和网络的具体优化算法及参数。对于框架的使用者而言，用户只需输入两个描述文件即可得到对该网络的优化结果，这无疑是非常方便的。
所以说，这是为了方便配置和使用caffe。

caffe框架选择使用谷歌的开源protobuf工具对这两部分进行描述，解析和存储，这一部分为caffe的">
<meta property="og:updated_time" content="2017-03-10T16:41:18.607Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Protobuf and caffe.proto">
<meta name="twitter:description" content="为什么要使用protobuf？一个好的软件框架应该要有明确的输入和输出，对于CNN网络而言，其主要有两部分组成：网络具体结构和网络的具体优化算法及参数。对于框架的使用者而言，用户只需输入两个描述文件即可得到对该网络的优化结果，这无疑是非常方便的。
所以说，这是为了方便配置和使用caffe。

caffe框架选择使用谷歌的开源protobuf工具对这两部分进行描述，解析和存储，这一部分为caffe的">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://verdin.cn/2017/03/10/Protobuf-and-caffe-proto/"/>





  <title> Protobuf and caffe.proto | Verdin小站 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?de255985a34f4b5d76b6cd2f11b8b565";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Verdin小站</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://verdin.cn/2017/03/10/Protobuf-and-caffe-proto/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Verdin">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Verdin小站">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Verdin小站" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Protobuf and caffe.proto
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-03-10T23:52:53+08:00">
                2017-03-10
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/03/10/Protobuf-and-caffe-proto/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/10/Protobuf-and-caffe-proto/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="为什么要使用protobuf？"><a href="#为什么要使用protobuf？" class="headerlink" title="为什么要使用protobuf？"></a>为什么要使用protobuf？</h2><p>一个好的软件框架应该要有<strong>明确的输入和输出</strong>，对于CNN网络而言，其主要有两部分组成：<strong>网络具体结构</strong>和<strong>网络的具体优化算法及参数</strong>。对于框架的使用者而言，用户只需输入两个描述文件即可得到对该网络的优化结果，这无疑是非常方便的。</p>
<p>所以说，这是为了方便配置和使用caffe。</p>
<hr>
<p>caffe框架选择使用谷歌的<strong>开源protobuf工具</strong>对这两部分进行<strong>描述，解析和存储</strong>，这一部分为caffe的实现节省了大量的代码。</p>
<p>例如目标检测demo，py-faster-rcnn，其主要分为训练和测试两个过程，两个过程的核心文件都是prototxt格式的文本文件。 </p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p><strong>输入：</strong></p>
<ol>
<li>slover.prototxt。描述网络训练时的各种参数文件，如训练的策略，学习率的变化率，模型保存的频率等参数。</li>
<li>train.prototxt。描述训练网络的网络结构文件。 </li>
<li>test.prototxt。描述测试网络的网络结构文件。 </li>
</ol>
<p><strong>输出：</strong></p>
<p>VGG16.caffemodel：保存的训练好的网络参数文件。</p>
<h2 id="protobuf的使用流程"><a href="#protobuf的使用流程" class="headerlink" title="protobuf的使用流程"></a>protobuf的使用流程</h2><p>protobuf工具主要是<strong>数据序列化存储和解析</strong>。在实际使用的时候主要是作为一个<strong>代码自动生成工具</strong>来使用，通过生成对所定义的数据结构的标准读写代码，用户可以通过标准的读写接口从文件中进行数据的读取，解析和存储。</p>
<p>目前proto支持C++，Python，Java等语言，这里主要演示caffe中使用的C++调用。</p>
<p><strong>主要使用过程为：</strong> </p>
<ol>
<li>编写XXX.proto文件。该文件里主要定义了各种数据结构及对应的数据类型，如int，string等。 </li>
<li>使用protoc对XXX.proto文件进行编译，生成对应的数据结构文件的读取和写入程序，程序接口都是标准化的。生成的文件一般名为XXX.pb.cc和XXX.pb.h。 </li>
<li>在新程序中使用XXX.pb.c和XXX.pb.h提供的代码。</li>
</ol>
<h2 id="简易caffe-proto编写解析示例"><a href="#简易caffe-proto编写解析示例" class="headerlink" title="简易caffe.proto编写解析示例"></a>简易caffe.proto编写解析示例</h2><p>为了后面更加清楚的理解protobuf工具，这里一个简单的caffe.proto为例进行<strong>solver.prototxt</strong>和<strong>train.prototxt</strong>的解析.</p>
<p><strong>caffe.proto文件编写：</strong><br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div><div class="line">333</div><div class="line">334</div><div class="line">335</div><div class="line">336</div><div class="line">337</div><div class="line">338</div><div class="line">339</div><div class="line">340</div><div class="line">341</div><div class="line">342</div><div class="line">343</div><div class="line">344</div><div class="line">345</div><div class="line">346</div><div class="line">347</div><div class="line">348</div><div class="line">349</div><div class="line">350</div><div class="line">351</div><div class="line">352</div><div class="line">353</div><div class="line">354</div><div class="line">355</div><div class="line">356</div><div class="line">357</div><div class="line">358</div><div class="line">359</div><div class="line">360</div><div class="line">361</div><div class="line">362</div><div class="line">363</div><div class="line">364</div><div class="line">365</div><div class="line">366</div><div class="line">367</div><div class="line">368</div><div class="line">369</div><div class="line">370</div><div class="line">371</div><div class="line">372</div><div class="line">373</div><div class="line">374</div><div class="line">375</div><div class="line">376</div><div class="line">377</div><div class="line">378</div><div class="line">379</div><div class="line">380</div><div class="line">381</div><div class="line">382</div><div class="line">383</div><div class="line">384</div><div class="line">385</div><div class="line">386</div><div class="line">387</div><div class="line">388</div><div class="line">389</div><div class="line">390</div><div class="line">391</div><div class="line">392</div><div class="line">393</div><div class="line">394</div><div class="line">395</div><div class="line">396</div><div class="line">397</div><div class="line">398</div><div class="line">399</div><div class="line">400</div><div class="line">401</div><div class="line">402</div><div class="line">403</div><div class="line">404</div><div class="line">405</div><div class="line">406</div><div class="line">407</div><div class="line">408</div><div class="line">409</div><div class="line">410</div><div class="line">411</div><div class="line">412</div><div class="line">413</div><div class="line">414</div><div class="line">415</div><div class="line">416</div><div class="line">417</div><div class="line">418</div><div class="line">419</div><div class="line">420</div><div class="line">421</div><div class="line">422</div><div class="line">423</div><div class="line">424</div><div class="line">425</div><div class="line">426</div><div class="line">427</div><div class="line">428</div><div class="line">429</div><div class="line">430</div><div class="line">431</div><div class="line">432</div><div class="line">433</div><div class="line">434</div><div class="line">435</div><div class="line">436</div><div class="line">437</div><div class="line">438</div><div class="line">439</div><div class="line">440</div><div class="line">441</div><div class="line">442</div><div class="line">443</div><div class="line">444</div><div class="line">445</div><div class="line">446</div><div class="line">447</div><div class="line">448</div><div class="line">449</div><div class="line">450</div><div class="line">451</div><div class="line">452</div><div class="line">453</div><div class="line">454</div><div class="line">455</div><div class="line">456</div><div class="line">457</div><div class="line">458</div><div class="line">459</div><div class="line">460</div><div class="line">461</div><div class="line">462</div><div class="line">463</div><div class="line">464</div><div class="line">465</div><div class="line">466</div><div class="line">467</div><div class="line">468</div><div class="line">469</div><div class="line">470</div><div class="line">471</div><div class="line">472</div><div class="line">473</div><div class="line">474</div><div class="line">475</div><div class="line">476</div><div class="line">477</div><div class="line">478</div><div class="line">479</div><div class="line">480</div><div class="line">481</div><div class="line">482</div><div class="line">483</div><div class="line">484</div><div class="line">485</div><div class="line">486</div><div class="line">487</div><div class="line">488</div><div class="line">489</div><div class="line">490</div><div class="line">491</div><div class="line">492</div><div class="line">493</div><div class="line">494</div><div class="line">495</div><div class="line">496</div><div class="line">497</div><div class="line">498</div><div class="line">499</div><div class="line">500</div><div class="line">501</div><div class="line">502</div><div class="line">503</div><div class="line">504</div><div class="line">505</div><div class="line">506</div><div class="line">507</div><div class="line">508</div><div class="line">509</div><div class="line">510</div><div class="line">511</div><div class="line">512</div><div class="line">513</div><div class="line">514</div><div class="line">515</div><div class="line">516</div><div class="line">517</div><div class="line">518</div><div class="line">519</div><div class="line">520</div><div class="line">521</div><div class="line">522</div><div class="line">523</div><div class="line">524</div><div class="line">525</div><div class="line">526</div><div class="line">527</div><div class="line">528</div><div class="line">529</div><div class="line">530</div><div class="line">531</div><div class="line">532</div><div class="line">533</div><div class="line">534</div><div class="line">535</div><div class="line">536</div><div class="line">537</div><div class="line">538</div><div class="line">539</div><div class="line">540</div><div class="line">541</div><div class="line">542</div><div class="line">543</div><div class="line">544</div><div class="line">545</div><div class="line">546</div><div class="line">547</div><div class="line">548</div><div class="line">549</div><div class="line">550</div><div class="line">551</div><div class="line">552</div><div class="line">553</div><div class="line">554</div><div class="line">555</div><div class="line">556</div><div class="line">557</div><div class="line">558</div><div class="line">559</div><div class="line">560</div><div class="line">561</div><div class="line">562</div><div class="line">563</div><div class="line">564</div><div class="line">565</div><div class="line">566</div><div class="line">567</div><div class="line">568</div><div class="line">569</div><div class="line">570</div><div class="line">571</div><div class="line">572</div><div class="line">573</div><div class="line">574</div><div class="line">575</div><div class="line">576</div><div class="line">577</div><div class="line">578</div><div class="line">579</div><div class="line">580</div><div class="line">581</div><div class="line">582</div><div class="line">583</div><div class="line">584</div><div class="line">585</div><div class="line">586</div><div class="line">587</div><div class="line">588</div><div class="line">589</div><div class="line">590</div><div class="line">591</div><div class="line">592</div><div class="line">593</div><div class="line">594</div><div class="line">595</div><div class="line">596</div><div class="line">597</div><div class="line">598</div><div class="line">599</div><div class="line">600</div><div class="line">601</div><div class="line">602</div><div class="line">603</div><div class="line">604</div><div class="line">605</div><div class="line">606</div><div class="line">607</div><div class="line">608</div><div class="line">609</div><div class="line">610</div><div class="line">611</div><div class="line">612</div><div class="line">613</div><div class="line">614</div><div class="line">615</div><div class="line">616</div><div class="line">617</div><div class="line">618</div><div class="line">619</div><div class="line">620</div><div class="line">621</div><div class="line">622</div><div class="line">623</div><div class="line">624</div><div class="line">625</div><div class="line">626</div><div class="line">627</div><div class="line">628</div><div class="line">629</div><div class="line">630</div><div class="line">631</div><div class="line">632</div><div class="line">633</div><div class="line">634</div><div class="line">635</div><div class="line">636</div><div class="line">637</div><div class="line">638</div><div class="line">639</div><div class="line">640</div><div class="line">641</div><div class="line">642</div><div class="line">643</div><div class="line">644</div><div class="line">645</div><div class="line">646</div><div class="line">647</div><div class="line">648</div><div class="line">649</div><div class="line">650</div><div class="line">651</div><div class="line">652</div><div class="line">653</div><div class="line">654</div><div class="line">655</div><div class="line">656</div><div class="line">657</div><div class="line">658</div><div class="line">659</div><div class="line">660</div><div class="line">661</div><div class="line">662</div><div class="line">663</div><div class="line">664</div><div class="line">665</div><div class="line">666</div><div class="line">667</div><div class="line">668</div><div class="line">669</div><div class="line">670</div><div class="line">671</div><div class="line">672</div><div class="line">673</div><div class="line">674</div><div class="line">675</div><div class="line">676</div><div class="line">677</div><div class="line">678</div><div class="line">679</div><div class="line">680</div><div class="line">681</div><div class="line">682</div><div class="line">683</div><div class="line">684</div><div class="line">685</div><div class="line">686</div><div class="line">687</div><div class="line">688</div><div class="line">689</div><div class="line">690</div><div class="line">691</div><div class="line">692</div><div class="line">693</div><div class="line">694</div><div class="line">695</div><div class="line">696</div><div class="line">697</div><div class="line">698</div><div class="line">699</div><div class="line">700</div><div class="line">701</div><div class="line">702</div><div class="line">703</div><div class="line">704</div><div class="line">705</div><div class="line">706</div><div class="line">707</div><div class="line">708</div><div class="line">709</div><div class="line">710</div><div class="line">711</div><div class="line">712</div><div class="line">713</div><div class="line">714</div><div class="line">715</div><div class="line">716</div><div class="line">717</div><div class="line">718</div><div class="line">719</div><div class="line">720</div><div class="line">721</div><div class="line">722</div><div class="line">723</div><div class="line">724</div><div class="line">725</div><div class="line">726</div><div class="line">727</div><div class="line">728</div><div class="line">729</div><div class="line">730</div><div class="line">731</div><div class="line">732</div><div class="line">733</div><div class="line">734</div><div class="line">735</div><div class="line">736</div><div class="line">737</div><div class="line">738</div><div class="line">739</div><div class="line">740</div><div class="line">741</div><div class="line">742</div><div class="line">743</div><div class="line">744</div><div class="line">745</div><div class="line">746</div><div class="line">747</div><div class="line">748</div><div class="line">749</div><div class="line">750</div><div class="line">751</div><div class="line">752</div><div class="line">753</div><div class="line">754</div><div class="line">755</div><div class="line">756</div><div class="line">757</div><div class="line">758</div><div class="line">759</div><div class="line">760</div><div class="line">761</div><div class="line">762</div><div class="line">763</div><div class="line">764</div><div class="line">765</div><div class="line">766</div><div class="line">767</div><div class="line">768</div><div class="line">769</div><div class="line">770</div><div class="line">771</div><div class="line">772</div><div class="line">773</div><div class="line">774</div><div class="line">775</div><div class="line">776</div><div class="line">777</div><div class="line">778</div><div class="line">779</div><div class="line">780</div><div class="line">781</div><div class="line">782</div><div class="line">783</div><div class="line">784</div><div class="line">785</div><div class="line">786</div><div class="line">787</div><div class="line">788</div><div class="line">789</div><div class="line">790</div><div class="line">791</div><div class="line">792</div><div class="line">793</div><div class="line">794</div><div class="line">795</div><div class="line">796</div><div class="line">797</div><div class="line">798</div><div class="line">799</div><div class="line">800</div><div class="line">801</div><div class="line">802</div><div class="line">803</div><div class="line">804</div><div class="line">805</div><div class="line">806</div><div class="line">807</div><div class="line">808</div><div class="line">809</div><div class="line">810</div><div class="line">811</div><div class="line">812</div><div class="line">813</div><div class="line">814</div><div class="line">815</div><div class="line">816</div><div class="line">817</div><div class="line">818</div><div class="line">819</div><div class="line">820</div><div class="line">821</div><div class="line">822</div><div class="line">823</div><div class="line">824</div><div class="line">825</div><div class="line">826</div><div class="line">827</div><div class="line">828</div><div class="line">829</div><div class="line">830</div><div class="line">831</div><div class="line">832</div><div class="line">833</div><div class="line">834</div><div class="line">835</div><div class="line">836</div><div class="line">837</div><div class="line">838</div><div class="line">839</div><div class="line">840</div><div class="line">841</div><div class="line">842</div><div class="line">843</div><div class="line">844</div><div class="line">845</div><div class="line">846</div><div class="line">847</div><div class="line">848</div><div class="line">849</div><div class="line">850</div><div class="line">851</div><div class="line">852</div><div class="line">853</div><div class="line">854</div><div class="line">855</div><div class="line">856</div><div class="line">857</div><div class="line">858</div><div class="line">859</div><div class="line">860</div><div class="line">861</div><div class="line">862</div><div class="line">863</div><div class="line">864</div><div class="line">865</div><div class="line">866</div><div class="line">867</div><div class="line">868</div><div class="line">869</div><div class="line">870</div><div class="line">871</div><div class="line">872</div><div class="line">873</div><div class="line">874</div><div class="line">875</div><div class="line">876</div><div class="line">877</div><div class="line">878</div><div class="line">879</div><div class="line">880</div><div class="line">881</div><div class="line">882</div><div class="line">883</div><div class="line">884</div><div class="line">885</div><div class="line">886</div><div class="line">887</div><div class="line">888</div><div class="line">889</div><div class="line">890</div><div class="line">891</div><div class="line">892</div><div class="line">893</div><div class="line">894</div><div class="line">895</div><div class="line">896</div><div class="line">897</div><div class="line">898</div><div class="line">899</div><div class="line">900</div><div class="line">901</div><div class="line">902</div><div class="line">903</div><div class="line">904</div><div class="line">905</div><div class="line">906</div><div class="line">907</div><div class="line">908</div><div class="line">909</div><div class="line">910</div><div class="line">911</div><div class="line">912</div><div class="line">913</div><div class="line">914</div><div class="line">915</div><div class="line">916</div><div class="line">917</div><div class="line">918</div><div class="line">919</div><div class="line">920</div><div class="line">921</div><div class="line">922</div><div class="line">923</div><div class="line">924</div><div class="line">925</div><div class="line">926</div><div class="line">927</div><div class="line">928</div><div class="line">929</div><div class="line">930</div><div class="line">931</div><div class="line">932</div><div class="line">933</div><div class="line">934</div><div class="line">935</div><div class="line">936</div><div class="line">937</div><div class="line">938</div><div class="line">939</div><div class="line">940</div><div class="line">941</div><div class="line">942</div><div class="line">943</div><div class="line">944</div><div class="line">945</div><div class="line">946</div><div class="line">947</div><div class="line">948</div><div class="line">949</div><div class="line">950</div><div class="line">951</div><div class="line">952</div><div class="line">953</div><div class="line">954</div><div class="line">955</div><div class="line">956</div><div class="line">957</div><div class="line">958</div><div class="line">959</div><div class="line">960</div><div class="line">961</div><div class="line">962</div><div class="line">963</div><div class="line">964</div><div class="line">965</div><div class="line">966</div><div class="line">967</div><div class="line">968</div><div class="line">969</div><div class="line">970</div><div class="line">971</div><div class="line">972</div><div class="line">973</div><div class="line">974</div><div class="line">975</div><div class="line">976</div><div class="line">977</div><div class="line">978</div><div class="line">979</div><div class="line">980</div><div class="line">981</div><div class="line">982</div><div class="line">983</div><div class="line">984</div><div class="line">985</div><div class="line">986</div><div class="line">987</div><div class="line">988</div><div class="line">989</div><div class="line">990</div><div class="line">991</div><div class="line">992</div><div class="line">993</div><div class="line">994</div><div class="line">995</div><div class="line">996</div><div class="line">997</div><div class="line">998</div><div class="line">999</div><div class="line">1000</div><div class="line">1001</div><div class="line">1002</div><div class="line">1003</div><div class="line">1004</div><div class="line">1005</div><div class="line">1006</div><div class="line">1007</div><div class="line">1008</div><div class="line">1009</div><div class="line">1010</div><div class="line">1011</div><div class="line">1012</div><div class="line">1013</div><div class="line">1014</div><div class="line">1015</div><div class="line">1016</div><div class="line">1017</div><div class="line">1018</div><div class="line">1019</div><div class="line">1020</div><div class="line">1021</div><div class="line">1022</div><div class="line">1023</div><div class="line">1024</div><div class="line">1025</div><div class="line">1026</div><div class="line">1027</div><div class="line">1028</div><div class="line">1029</div><div class="line">1030</div><div class="line">1031</div><div class="line">1032</div><div class="line">1033</div><div class="line">1034</div><div class="line">1035</div><div class="line">1036</div><div class="line">1037</div><div class="line">1038</div><div class="line">1039</div><div class="line">1040</div><div class="line">1041</div><div class="line">1042</div><div class="line">1043</div><div class="line">1044</div><div class="line">1045</div><div class="line">1046</div><div class="line">1047</div><div class="line">1048</div><div class="line">1049</div><div class="line">1050</div><div class="line">1051</div><div class="line">1052</div><div class="line">1053</div><div class="line">1054</div><div class="line">1055</div><div class="line">1056</div><div class="line">1057</div><div class="line">1058</div><div class="line">1059</div><div class="line">1060</div><div class="line">1061</div><div class="line">1062</div><div class="line">1063</div><div class="line">1064</div><div class="line">1065</div><div class="line">1066</div><div class="line">1067</div><div class="line">1068</div><div class="line">1069</div><div class="line">1070</div><div class="line">1071</div><div class="line">1072</div><div class="line">1073</div><div class="line">1074</div><div class="line">1075</div><div class="line">1076</div><div class="line">1077</div><div class="line">1078</div><div class="line">1079</div><div class="line">1080</div><div class="line">1081</div><div class="line">1082</div><div class="line">1083</div><div class="line">1084</div><div class="line">1085</div><div class="line">1086</div><div class="line">1087</div><div class="line">1088</div><div class="line">1089</div><div class="line">1090</div><div class="line">1091</div><div class="line">1092</div><div class="line">1093</div><div class="line">1094</div><div class="line">1095</div><div class="line">1096</div><div class="line">1097</div><div class="line">1098</div><div class="line">1099</div><div class="line">1100</div><div class="line">1101</div><div class="line">1102</div><div class="line">1103</div><div class="line">1104</div><div class="line">1105</div><div class="line">1106</div><div class="line">1107</div><div class="line">1108</div><div class="line">1109</div><div class="line">1110</div><div class="line">1111</div><div class="line">1112</div><div class="line">1113</div><div class="line">1114</div><div class="line">1115</div><div class="line">1116</div><div class="line">1117</div><div class="line">1118</div><div class="line">1119</div><div class="line">1120</div><div class="line">1121</div><div class="line">1122</div><div class="line">1123</div><div class="line">1124</div><div class="line">1125</div><div class="line">1126</div><div class="line">1127</div><div class="line">1128</div><div class="line">1129</div><div class="line">1130</div><div class="line">1131</div><div class="line">1132</div><div class="line">1133</div><div class="line">1134</div><div class="line">1135</div><div class="line">1136</div><div class="line">1137</div><div class="line">1138</div><div class="line">1139</div><div class="line">1140</div><div class="line">1141</div><div class="line">1142</div><div class="line">1143</div><div class="line">1144</div><div class="line">1145</div><div class="line">1146</div><div class="line">1147</div><div class="line">1148</div><div class="line">1149</div><div class="line">1150</div><div class="line">1151</div><div class="line">1152</div><div class="line">1153</div><div class="line">1154</div><div class="line">1155</div><div class="line">1156</div><div class="line">1157</div><div class="line">1158</div><div class="line">1159</div><div class="line">1160</div><div class="line">1161</div><div class="line">1162</div><div class="line">1163</div><div class="line">1164</div><div class="line">1165</div><div class="line">1166</div><div class="line">1167</div><div class="line">1168</div><div class="line">1169</div><div class="line">1170</div><div class="line">1171</div><div class="line">1172</div><div class="line">1173</div><div class="line">1174</div><div class="line">1175</div><div class="line">1176</div><div class="line">1177</div><div class="line">1178</div><div class="line">1179</div><div class="line">1180</div><div class="line">1181</div><div class="line">1182</div><div class="line">1183</div><div class="line">1184</div><div class="line">1185</div><div class="line">1186</div><div class="line">1187</div><div class="line">1188</div><div class="line">1189</div><div class="line">1190</div><div class="line">1191</div><div class="line">1192</div><div class="line">1193</div><div class="line">1194</div><div class="line">1195</div><div class="line">1196</div><div class="line">1197</div><div class="line">1198</div><div class="line">1199</div><div class="line">1200</div><div class="line">1201</div><div class="line">1202</div><div class="line">1203</div><div class="line">1204</div><div class="line">1205</div><div class="line">1206</div><div class="line">1207</div><div class="line">1208</div><div class="line">1209</div><div class="line">1210</div><div class="line">1211</div><div class="line">1212</div><div class="line">1213</div><div class="line">1214</div><div class="line">1215</div><div class="line">1216</div><div class="line">1217</div><div class="line">1218</div><div class="line">1219</div><div class="line">1220</div><div class="line">1221</div><div class="line">1222</div><div class="line">1223</div><div class="line">1224</div><div class="line">1225</div><div class="line">1226</div><div class="line">1227</div><div class="line">1228</div><div class="line">1229</div><div class="line">1230</div><div class="line">1231</div><div class="line">1232</div><div class="line">1233</div><div class="line">1234</div><div class="line">1235</div><div class="line">1236</div><div class="line">1237</div><div class="line">1238</div><div class="line">1239</div><div class="line">1240</div><div class="line">1241</div><div class="line">1242</div><div class="line">1243</div><div class="line">1244</div><div class="line">1245</div><div class="line">1246</div><div class="line">1247</div><div class="line">1248</div><div class="line">1249</div><div class="line">1250</div><div class="line">1251</div><div class="line">1252</div><div class="line">1253</div><div class="line">1254</div><div class="line">1255</div><div class="line">1256</div><div class="line">1257</div><div class="line">1258</div><div class="line">1259</div><div class="line">1260</div><div class="line">1261</div><div class="line">1262</div><div class="line">1263</div><div class="line">1264</div><div class="line">1265</div><div class="line">1266</div><div class="line">1267</div><div class="line">1268</div><div class="line">1269</div><div class="line">1270</div><div class="line">1271</div><div class="line">1272</div><div class="line">1273</div><div class="line">1274</div><div class="line">1275</div><div class="line">1276</div><div class="line">1277</div><div class="line">1278</div><div class="line">1279</div><div class="line">1280</div><div class="line">1281</div><div class="line">1282</div><div class="line">1283</div><div class="line">1284</div><div class="line">1285</div><div class="line">1286</div><div class="line">1287</div><div class="line">1288</div><div class="line">1289</div><div class="line">1290</div><div class="line">1291</div><div class="line">1292</div><div class="line">1293</div><div class="line">1294</div><div class="line">1295</div><div class="line">1296</div><div class="line">1297</div><div class="line">1298</div><div class="line">1299</div><div class="line">1300</div><div class="line">1301</div><div class="line">1302</div><div class="line">1303</div><div class="line">1304</div><div class="line">1305</div><div class="line">1306</div><div class="line">1307</div><div class="line">1308</div><div class="line">1309</div><div class="line">1310</div><div class="line">1311</div><div class="line">1312</div><div class="line">1313</div><div class="line">1314</div><div class="line">1315</div><div class="line">1316</div><div class="line">1317</div><div class="line">1318</div><div class="line">1319</div><div class="line">1320</div><div class="line">1321</div><div class="line">1322</div><div class="line">1323</div><div class="line">1324</div><div class="line">1325</div><div class="line">1326</div><div class="line">1327</div><div class="line">1328</div><div class="line">1329</div><div class="line">1330</div><div class="line">1331</div><div class="line">1332</div><div class="line">1333</div><div class="line">1334</div><div class="line">1335</div><div class="line">1336</div><div class="line">1337</div><div class="line">1338</div><div class="line">1339</div><div class="line">1340</div><div class="line">1341</div><div class="line">1342</div><div class="line">1343</div><div class="line">1344</div><div class="line">1345</div><div class="line">1346</div><div class="line">1347</div><div class="line">1348</div><div class="line">1349</div><div class="line">1350</div><div class="line">1351</div><div class="line">1352</div><div class="line">1353</div><div class="line">1354</div><div class="line">1355</div><div class="line">1356</div><div class="line">1357</div><div class="line">1358</div><div class="line">1359</div><div class="line">1360</div><div class="line">1361</div><div class="line">1362</div><div class="line">1363</div><div class="line">1364</div><div class="line">1365</div><div class="line">1366</div><div class="line">1367</div><div class="line">1368</div><div class="line">1369</div><div class="line">1370</div><div class="line">1371</div><div class="line">1372</div><div class="line">1373</div><div class="line">1374</div><div class="line">1375</div><div class="line">1376</div><div class="line">1377</div><div class="line">1378</div><div class="line">1379</div><div class="line">1380</div><div class="line">1381</div><div class="line">1382</div><div class="line">1383</div><div class="line">1384</div><div class="line">1385</div><div class="line">1386</div><div class="line">1387</div><div class="line">1388</div><div class="line">1389</div><div class="line">1390</div><div class="line">1391</div><div class="line">1392</div><div class="line">1393</div><div class="line">1394</div><div class="line">1395</div><div class="line">1396</div><div class="line">1397</div><div class="line">1398</div><div class="line">1399</div><div class="line">1400</div><div class="line">1401</div><div class="line">1402</div><div class="line">1403</div><div class="line">1404</div><div class="line">1405</div><div class="line">1406</div><div class="line">1407</div><div class="line">1408</div><div class="line">1409</div><div class="line">1410</div><div class="line">1411</div><div class="line">1412</div><div class="line">1413</div><div class="line">1414</div><div class="line">1415</div><div class="line">1416</div><div class="line">1417</div><div class="line">1418</div><div class="line">1419</div><div class="line">1420</div><div class="line">1421</div><div class="line">1422</div><div class="line">1423</div><div class="line">1424</div><div class="line">1425</div><div class="line">1426</div><div class="line">1427</div><div class="line">1428</div><div class="line">1429</div><div class="line">1430</div><div class="line">1431</div><div class="line">1432</div><div class="line">1433</div><div class="line">1434</div><div class="line">1435</div><div class="line">1436</div><div class="line">1437</div><div class="line">1438</div><div class="line">1439</div><div class="line">1440</div><div class="line">1441</div><div class="line">1442</div><div class="line">1443</div><div class="line">1444</div><div class="line">1445</div><div class="line">1446</div><div class="line">1447</div><div class="line">1448</div><div class="line">1449</div><div class="line">1450</div><div class="line">1451</div><div class="line">1452</div><div class="line">1453</div><div class="line">1454</div><div class="line">1455</div><div class="line">1456</div><div class="line">1457</div><div class="line">1458</div><div class="line">1459</div><div class="line">1460</div><div class="line">1461</div><div class="line">1462</div><div class="line">1463</div><div class="line">1464</div><div class="line">1465</div><div class="line">1466</div><div class="line">1467</div><div class="line">1468</div><div class="line">1469</div><div class="line">1470</div><div class="line">1471</div><div class="line">1472</div><div class="line">1473</div><div class="line">1474</div><div class="line">1475</div><div class="line">1476</div><div class="line">1477</div><div class="line">1478</div><div class="line">1479</div><div class="line">1480</div><div class="line">1481</div><div class="line">1482</div><div class="line">1483</div><div class="line">1484</div><div class="line">1485</div><div class="line">1486</div><div class="line">1487</div><div class="line">1488</div><div class="line">1489</div><div class="line">1490</div><div class="line">1491</div><div class="line">1492</div><div class="line">1493</div><div class="line">1494</div><div class="line">1495</div><div class="line">1496</div><div class="line">1497</div><div class="line">1498</div><div class="line">1499</div><div class="line">1500</div></pre></td><td class="code"><pre><div class="line">//////////////////</div><div class="line">caffe.proto文件注释，</div><div class="line">caffe版本:MS-caffe-master github 2016.8.20</div><div class="line">caffe版本:BVLC-caffe-master github 2016.8.20</div><div class="line">//////////////////</div><div class="line">syntax = "proto2";</div><div class="line">package caffe;  </div><div class="line">// 数据块形状&#123;指定Blob的形状或维度-4D&#125;</div><div class="line">message BlobShape &#123;</div><div class="line">  //数据块形状定义为Num×Channel×Height×Wight原因在于caffe基于容器的多维嵌套</div><div class="line">  //来实现高维数据的封装。即vector(N)&gt;。</div><div class="line">  repeated int64 dim = 1 [packed = true];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 数据块&#123;形状，数据，微分&#125;</div><div class="line">message BlobProto &#123;</div><div class="line">  optional BlobShape shape = 7;</div><div class="line">  repeated float data = 5 [packed = true];</div><div class="line">  repeated float diff = 6 [packed = true];</div><div class="line">  repeated double double_data = 8 [packed = true];</div><div class="line">  repeated double double_diff = 9 [packed = true];</div><div class="line"></div><div class="line">  //数据4D形状 -- 旧版本，已使用"BlobShape shape"代替：</div><div class="line">  optional int32 num = 1 [default = 0]; //样本</div><div class="line">  optional int32 channels = 2 [default = 0];</div><div class="line">  optional int32 height = 3 [default = 0];</div><div class="line">  optional int32 width = 4 [default = 0];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 存放多个BlobProto实例的对应Index，易于引用</div><div class="line">message BlobProtoVector &#123;</div><div class="line">  repeated BlobProto blobs = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 数据:&#123;C,H,W,data(uchar&amp;float),label&#125; 图像样本</div><div class="line">message Datum &#123;</div><div class="line">  optional int32 channels = 1;</div><div class="line">  optional int32 height = 2;</div><div class="line">  optional int32 width = 3;</div><div class="line">  // the actual image data, in bytes</div><div class="line">  optional bytes data = 4;</div><div class="line">  optional int32 label = 5;</div><div class="line">  // Optionally, the datum could also hold float data.</div><div class="line">  repeated float float_data = 6;</div><div class="line">  // If true data contains an encoded image that need to be decoded</div><div class="line">  optional bool encoded = 7 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">//滤波器参数&#123;Type(const|uniform|gauss),&#125;</div><div class="line">message FillerParameter &#123;</div><div class="line">  // The filler type.</div><div class="line">  optional string type = 1 [default = 'constant'];</div><div class="line">  optional float value = 2 [default = 0]; // the value in constant filler</div><div class="line">  optional float min = 3 [default = 0]; // the min value in uniform filler</div><div class="line">  optional float max = 4 [default = 1]; // the max value in uniform filler</div><div class="line">  optional float mean = 5 [default = 0]; // the mean value in Gaussian filler</div><div class="line">  optional float std = 6 [default = 1]; // the std value in Gaussian filler</div><div class="line">  // 给定输入与权值相乘后应该得到非零输出，默认值-1意为不稀疏化高斯模板。</div><div class="line">  optional int32 sparse = 7 [default = -1];</div><div class="line">  // Normalize the filler variance by fan_in, fan_out, or their average.</div><div class="line">  // Applies to 'xavier' and 'msra' fillers.（扇入，扇出）</div><div class="line">  // 通过fanIn,fanOut,及其均值来归一化填充值的方差，有“xavier法”或“msra法”</div><div class="line">  enum VarianceNorm &#123;</div><div class="line">    FAN_IN = 0;</div><div class="line">    FAN_OUT = 1;</div><div class="line">    AVERAGE = 2;</div><div class="line">  &#125;</div><div class="line">  optional VarianceNorm variance_norm = 8 [default = FAN_IN];</div><div class="line">&#125;</div><div class="line"></div><div class="line">//网络参数&#123;网名，输入参数，数据块形状，forceBack，NetState,debugInfo,&#125;</div><div class="line">message NetParameter &#123;</div><div class="line">  optional string name = 1; // consider giving the network a name</div><div class="line">  // 旧版--输入网络的数据块Blobs; 改为新版--InputParameter</div><div class="line">  repeated string input = 3;</div><div class="line">  // DEPRECATED. See InputParameter. The shape of the input blobs.</div><div class="line">  // 旧版--输入的Blobs的形状； 改为新版--InputerParameter</div><div class="line">  repeated BlobShape input_shape = 8;</div><div class="line"></div><div class="line">  // 指定Blobs的4D输入形状 -- 已改为新版：input_shape代替</div><div class="line">  // 如要使用旧版，对每个输入的blob都需要指定4个参数，Num×Cha×H×W</div><div class="line">  // 因此 input_dim需要重复4次</div><div class="line">  repeated int32 input_dim = 4;</div><div class="line"></div><div class="line">  //确定网络是否要让每个层都强制反向传播。</div><div class="line">  //如果设置为false,将根据网络结构和学习率来自动确定是否需要反向传播。</div><div class="line">  //网络的当前状态"state"包括"phase","level","stage"。(???)</div><div class="line">  //某些层需要设置phase属性，使其跳过网络运行时的某些状态.</div><div class="line">  optional NetState state = 6;</div><div class="line"></div><div class="line">  // 当运行Net::Forward/Backward/Update时，打印调试信息，默认false.</div><div class="line">  optional bool debug_info = 7 [default = false];</div><div class="line"></div><div class="line">  // 构成net的layers。每个layer的链接和行为通过LayerParameter配置。</div><div class="line">  repeated LayerParameter layer = 100;  // ID 100 so layers are printed last.</div><div class="line"></div><div class="line">  // DEPRECATED: use 'layer' instead.</div><div class="line">  repeated V1LayerParameter layers = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// NOTE：注意</div><div class="line">// Update the next available ID when you add a new SolverParameter field.</div><div class="line">// 当你添加一个新的SolverParameter属性时，需要更新下一个可获得的ID</div><div class="line">// SolverParameter next available ID: 41 (last added: type)</div><div class="line"></div><div class="line">//求解器参数&#123;网络，&#125;</div><div class="line">message SolverParameter &#123;</div><div class="line">  //////////////////////////////////////////////////////////////////////////////</div><div class="line">  // Specifying the train and test networks</div><div class="line">  //</div><div class="line">  // Exactly one train net must be specified using one of the following fields:</div><div class="line">  //     train_net_param, train_net, net_param, net</div><div class="line">  // One or more test nets may be specified using any of the following fields:</div><div class="line">  //     test_net_param, test_net, net_param, net</div><div class="line">  // If more than one test net field is specified (e.g., both net and</div><div class="line">  // test_net are specified), they will be evaluated in the field order given</div><div class="line">  // above: (1) test_net_param, (2) test_net, (3) net_param/net.</div><div class="line">  // A test_iter must be specified for each test_net.</div><div class="line">  // A test_level and/or a test_stage may also be specified for each test_net.</div><div class="line">  //////////////////////////////////////////////////////////////////////////////</div><div class="line">  //指定网络，可有以下的多种形式</div><div class="line">  // Proto filename for the train net, possibly combined with one or more</div><div class="line">  // test nets.</div><div class="line">  optional string net = 24;</div><div class="line">  // Inline train net param, possibly combined with one or more test nets.</div><div class="line">  optional NetParameter net_param = 25;</div><div class="line"></div><div class="line">  optional string train_net = 1; // Proto filename for the train net.</div><div class="line">  repeated string test_net = 2; // Proto filenames for the test nets.</div><div class="line">  optional NetParameter train_net_param = 21; // Inline train net params.</div><div class="line">  repeated NetParameter test_net_param = 22; // Inline test net params.</div><div class="line"></div><div class="line"> // 指定网络状态</div><div class="line"> // The states for the train/test nets. Must be unspecified or</div><div class="line">  // specified once per net.</div><div class="line">  //</div><div class="line">  // By default, all states will have solver = true;</div><div class="line">  // train_state will have phase = TRAIN,</div><div class="line">  // and all test_state's will have phase = TEST.</div><div class="line">  // Other defaults are set according to the NetState defaults.</div><div class="line">  optional NetState train_state = 26;</div><div class="line">  repeated NetState test_state = 27;</div><div class="line"></div><div class="line">  //测试迭代批次数：</div><div class="line">  //合理设置可使得测试遍历完全部测试样本</div><div class="line">  //合理值 = 测试样本总数/每批次测试数 = totalTestSamples/batchSize</div><div class="line">  repeated int32 test_iter = 3;</div><div class="line"></div><div class="line">  //训练迭代批次数：</div><div class="line">  //两次测试之间所经历的训练迭代次数:合理设置可使得训练遍历完全部训练样本</div><div class="line">  //合理值 = 训练样本总数/每批次训练数 = totalTrainSamples/batchSize</div><div class="line">  optional int32 test_interval = 4 [default = 0];</div><div class="line">  //训练test_interval个批次，再测试test_iter个批次，为一个回合(epoch)</div><div class="line">  //合理设置应使得每个回合内，遍历覆盖到全部训练样本和测试样本</div><div class="line"></div><div class="line">  //默认不计算测试时损失</div><div class="line">  optional bool test_compute_loss = 19 [default = false];</div><div class="line"></div><div class="line">  // 如设置为真，则在训练前运行一次测试，以确保内存足够，并打印初始损失值</div><div class="line">  optional bool test_initialization = 32 [default = true];</div><div class="line">  // 基本学习速率</div><div class="line">  optional float base_lr = 5; // The base learning rate</div><div class="line">  // 打印信息的遍历间隔，遍历多少个批次打印一次信息。设置为0则不打印。</div><div class="line">  optional int32 display = 6;</div><div class="line">  // Display the loss averaged over the last average_loss iterations</div><div class="line">  // 打印最后一个迭代批次下的平均损失（？）</div><div class="line">  optional int32 average_loss = 33 [default = 1];</div><div class="line">  // 训练最大迭代次数</div><div class="line">  optional int32 max_iter = 7;</div><div class="line">  // accumulate gradients over `iter_size` x `batch_size` instances</div><div class="line">  // 累积梯度误差基于“iter_size×batchSize”个样本实例</div><div class="line">  // “批次数×批量数”=“遍历的批次数×每批的样本数”个样本实例</div><div class="line">  optional int32 iter_size = 36 [default = 1];</div><div class="line"></div><div class="line">  //学习率衰减策略(7种)</div><div class="line">  // The learning rate decay policy. The currently implemented learning rate</div><div class="line">  // policies are as follows:</div><div class="line">  //    - fixed: always return base_lr.</div><div class="line">  //    - step: return base_lr * gamma ^ (floor(iter / step))</div><div class="line">  //    - exp: return base_lr * gamma ^ iter</div><div class="line">  //    - inv: return base_lr * (1 + gamma * iter) ^ (- power)</div><div class="line">  //    - multistep: similar to step butallows non uniform steps defined by</div><div class="line">  //      stepvalue</div><div class="line">  //    - poly: the effective learning rate follows a polynomial decay, to be</div><div class="line">  //      zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)</div><div class="line">  //    - sigmoid: the effective learning rate follows a sigmod decay</div><div class="line">  //      return base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))</div><div class="line">  //</div><div class="line">  // 在上述参数中，base_lr, max_iter, gamma, step, stepvalue and power 被定义</div><div class="line">  // 在solver.prototxt文件中，iter是当前迭代次数。</div><div class="line">  optional string lr_policy = 8; //学习率调节策略</div><div class="line">  optional float gamma = 9; // The parameter to compute the learning rate.</div><div class="line">  optional float power = 10; // The parameter to compute the learning rate.</div><div class="line">  optional float momentum = 11; // The momentum value.动量</div><div class="line">  optional float weight_decay = 12; // The weight decay.权值衰减系数</div><div class="line">  //由权值衰减系数所控制的正则化类型：L1或L2范数，默认L2</div><div class="line">  optional string regularization_type = 29 [default = "L2"];</div><div class="line">  //"step"策略下，学习率的步长值</div><div class="line">  optional int32 stepsize = 13;</div><div class="line">  //"multistep"策略下的步长值</div><div class="line">  repeated int32 stepvalue = 34;</div><div class="line"></div><div class="line">  //设置梯度裁剪阈值为&gt;=0，当其实际L2范数超出此值时(?)</div><div class="line">  optional float clip_gradients = 35 [default = -1];</div><div class="line"></div><div class="line">  //快照间隔，遍历多少次对模型和求解器状态保存一次</div><div class="line">  optional int32 snapshot = 14 [default = 0]; // The snapshot interval</div><div class="line">  optional string snapshot_prefix = 15; // The prefix for the snapshot.</div><div class="line">  //是否对diff快照，有助调试，但最终的protocol buffer尺寸会很大</div><div class="line">  optional bool snapshot_diff = 16 [default = false];</div><div class="line">  //快照数据保存格式&#123;hdf5,binaryproto(默认)&#125;</div><div class="line">  enum SnapshotFormat &#123;</div><div class="line">    HDF5 = 0;</div><div class="line">    BINARYPROTO = 1;</div><div class="line">  &#125;</div><div class="line">  optional SnapshotFormat snapshot_format = 37 [default = BINARYPROTO];</div><div class="line">  // the mode solver will use: 0 for CPU and 1 for GPU. Use GPU in default.</div><div class="line">  enum SolverMode &#123;</div><div class="line">    CPU = 0;</div><div class="line">    GPU = 1;</div><div class="line">  &#125;</div><div class="line">  求解模式&#123;GPU(device_id),CPU&#125;</div><div class="line">  optional SolverMode solver_mode = 17 [default = GPU];</div><div class="line">  optional int32 device_id = 18 [default = 0];</div><div class="line">  //随机数种子，设为正则表示Solver会以此为随机数初始化caffe,可产生重复随机</div><div class="line">  //数，易于重复试验；设为默认-1代表使用系统时钟作为种子。</div><div class="line">  optional int64 random_seed = 20 [default = -1];</div><div class="line"></div><div class="line">  //求解器类型=SGD(默认)</div><div class="line">  optional string type = 40 [default = "SGD"];</div><div class="line"></div><div class="line">  // numerical stability for RMSProp, AdaGrad and AdaDelta and Adam</div><div class="line">  optional float delta = 31 [default = 1e-8];</div><div class="line">  // parameters for the Adam solver</div><div class="line">  optional float momentum2 = 39 [default = 0.999];</div><div class="line"></div><div class="line">  // RMSProp decay value</div><div class="line">  // MeanSquare(t) = rms_decay*MeanSquare(t-1) + (1-rms_decay)*SquareGradient(t)</div><div class="line">  optional float rms_decay = 38;</div><div class="line"></div><div class="line">  //若真，则打印网络状态信息，有助于调试问题</div><div class="line">  optional bool debug_info = 23 [default = false];</div><div class="line"></div><div class="line">  //若假，则不会在训练后保存快照</div><div class="line">  optional bool snapshot_after_train = 28 [default = true];</div><div class="line"></div><div class="line">  // DEPRECATED: old solver enum types, use string instead</div><div class="line">  enum SolverType &#123;</div><div class="line">    SGD = 0;</div><div class="line">    NESTEROV = 1;</div><div class="line">    ADAGRAD = 2;</div><div class="line">    RMSPROP = 3;</div><div class="line">    ADADELTA = 4;</div><div class="line">    ADAM = 5;</div><div class="line">  &#125;</div><div class="line">  // DEPRECATED: use type instead of solver_type</div><div class="line">  optional SolverType solver_type = 30 [default = SGD];</div><div class="line">&#125;</div><div class="line"></div><div class="line">//对求解器状态进行快照的消息</div><div class="line">message SolverState &#123;</div><div class="line">  optional int32 iter = 1; // The current iteration</div><div class="line">  optional string learned_net = 2; // The file that stores the learned net.</div><div class="line">  repeated BlobProto history = 3; // The history for sgd solvers</div><div class="line">  optional int32 current_step = 4 [default = 0]; // The current step for learning rate</div><div class="line">&#125;</div><div class="line"></div><div class="line">enum Phase &#123;</div><div class="line">   TRAIN = 0;</div><div class="line">   TEST = 1;</div><div class="line">&#125;</div><div class="line">//NetState&#123;phase,level,stage&#125;</div><div class="line">message NetState &#123;</div><div class="line">  optional Phase phase = 1 [default = TEST];</div><div class="line">  optional int32 level = 2 [default = 0];</div><div class="line">  repeated string stage = 3;</div><div class="line">&#125;</div><div class="line"></div><div class="line">//网络状态规则&#123;phases,levels,stages&#125;</div><div class="line">message NetStateRule &#123;</div><div class="line">  //在NetState中设置phase值(TRAIN|TEST)，使其符合此规则</div><div class="line">  optional Phase phase = 1;</div><div class="line"></div><div class="line">  //设置layer中所使用的最小最大levels。使其不定义以满足忽视level的规则。</div><div class="line">  optional int32 min_level = 2;</div><div class="line">  optional int32 max_level = 3;</div><div class="line"></div><div class="line">  // Customizable sets of stages to include or exclude.</div><div class="line">  // The net must have ALL of the specified stages and NONE of the specified</div><div class="line">  // "not_stage"s to meet the rule.</div><div class="line">  // (Use multiple NetStateRules to specify conjunctions of stages.)</div><div class="line">  //可定制的stages集合，用于include或exclude在网络中。网络必须包含全</div><div class="line">  //部制定的"stages"或不包含全部制定的"not_stage"</div><div class="line">  repeated string stage = 4;</div><div class="line">  repeated string not_stage = 5;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Specifies training parameters (multipliers on global learning constants,</div><div class="line">// and the name and other settings used for weight sharing).</div><div class="line">//指定训练参数(乘数及全局学习率常数)和其名称，以及其他用于权值共享的设置。</div><div class="line">message ParamSpec &#123;</div><div class="line">  // 设定参数blobs的名称--用于在层间共享参数，若无此需求则不用设计。</div><div class="line">  optional string name = 1;</div><div class="line"></div><div class="line">  //共享权重时是否需要其形状相同或仅仅数量相同，默认为形状相同</div><div class="line">  optional DimCheckMode share_mode = 2;</div><div class="line">  enum DimCheckMode &#123;</div><div class="line">    // STRICT (default) 形状相同(num, channels, height, width)都匹配.</div><div class="line">    STRICT = 0;</div><div class="line">    // PERMISSIVE 数量相同</div><div class="line">    PERMISSIVE = 1;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  // The multiplier on the global learning rate for this parameter.</div><div class="line">  // 全局学习率的乘数</div><div class="line">  optional float lr_mult = 3 [default = 1.0];</div><div class="line"></div><div class="line">  // The multiplier on the global weight decay for this parameter.</div><div class="line">  // 全局权值衰减系数的乘数</div><div class="line">  optional float decay_mult = 4 [default = 1.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line">//注意：</div><div class="line">//当在LayerParameter中新增字段时，需要为其更新下一个可用ID。</div><div class="line">//比如，最近新增了smooth_l1_loss_param层，则为其指定层专属ID:149。</div><div class="line">//层参数&#123;名称，类型，输入底，输出顶，阶段，损失加权系数，全局乘数，&#125;</div><div class="line">message LayerParameter &#123;</div><div class="line">  optional string name = 1; // 类名称</div><div class="line">  optional string type = 2; // 类类型</div><div class="line">  repeated string bottom = 3; // the name of each bottom blob 输入blob名称</div><div class="line">  repeated string top = 4; // the name of each top blob 输出blob名称</div><div class="line"></div><div class="line">  // The train / test phase for computation. //阶段，运行时状态</div><div class="line">  optional Phase phase = 10;</div><div class="line"></div><div class="line">  //每层输出blob在目标损失函数中的加权系数，每层默认为0或1</div><div class="line">  repeated float loss_weight = 5;</div><div class="line"></div><div class="line">  //指定训练参数(全局学习率上的乘数lr_mrlt)</div><div class="line">  repeated ParamSpec param = 6;</div><div class="line"></div><div class="line"></div><div class="line">  // The blobs containing the numeric parameters of the layer.</div><div class="line">  //包含每层数值参数的blobs</div><div class="line">  repeated BlobProto blobs = 7;</div><div class="line"></div><div class="line">  // Specifies whether to backpropagate to each bottom. If unspecified,</div><div class="line">  // Caffe will automatically infer whether each input needs backpropagation</div><div class="line">  // to compute parameter gradients. If set to true for some inputs,</div><div class="line">  // backpropagation to those inputs is forced; if set false for some inputs,</div><div class="line">  // backpropagation to those inputs is skipped.</div><div class="line">  //</div><div class="line">  // The size must be either 0 or equal to the number of bottoms.</div><div class="line"></div><div class="line">  repeated bool propagate_down = 11;</div><div class="line"></div><div class="line">  // Rules控制每层是否被包含在网络中，基于当前的NetState. 可使用非0数规则来</div><div class="line">  // include或exclude，但不能兼有。如果未指定include或exclude规则，则该层总是</div><div class="line">  // 被包含在内。</div><div class="line">  repeated NetStateRule include = 8;</div><div class="line">  repeated NetStateRule exclude = 9;</div><div class="line"></div><div class="line">  // 用于数据预处理的参数</div><div class="line">  optional TransformationParameter transform_param = 100;</div><div class="line"></div><div class="line">  // 由loss层共享的参数.</div><div class="line">  optional LossParameter loss_param = 101;</div><div class="line"></div><div class="line">  // Layer type-specific parameters.</div><div class="line">  //</div><div class="line">  // Note: certain layers may have more than one computational engine</div><div class="line">  // for their implementation. These layers include an Engine type and</div><div class="line">  // engine parameter for selecting the implementation.</div><div class="line">  // The default for the engine is set by the ENGINE switch at compile-time.</div><div class="line">  // 层类型指定参数</div><div class="line">  // 注意：</div><div class="line">  optional AccuracyParameter accuracy_param = 102;</div><div class="line">  optional ArgMaxParameter argmax_param = 103;</div><div class="line">  optional BatchNormParameter batch_norm_param = 139;</div><div class="line">  optional BiasParameter bias_param = 141;</div><div class="line">  optional ConcatParameter concat_param = 104;</div><div class="line">  optional ContrastiveLossParameter contrastive_loss_param = 105;</div><div class="line">  optional ConvolutionParameter convolution_param = 106;</div><div class="line">  optional CropParameter crop_param = 144;</div><div class="line">  optional DataParameter data_param = 107;</div><div class="line">  optional DropoutParameter dropout_param = 108;</div><div class="line">  optional DummyDataParameter dummy_data_param = 109;</div><div class="line">  optional EltwiseParameter eltwise_param = 110;</div><div class="line">  optional ELUParameter elu_param = 140;</div><div class="line">  optional EmbedParameter embed_param = 137;</div><div class="line">  optional ExpParameter exp_param = 111;</div><div class="line">  optional FlattenParameter flatten_param = 135;</div><div class="line">  optional HDF5DataParameter hdf5_data_param = 112;</div><div class="line">  optional HDF5OutputParameter hdf5_output_param = 113;</div><div class="line">  optional HingeLossParameter hinge_loss_param = 114;</div><div class="line">  optional ImageDataParameter image_data_param = 115;</div><div class="line">  optional InfogainLossParameter infogain_loss_param = 116;</div><div class="line">  optional InnerProductParameter inner_product_param = 117;</div><div class="line">  optional InputParameter input_param = 143;</div><div class="line">  optional LogParameter log_param = 134;</div><div class="line">  optional LRNParameter lrn_param = 118;</div><div class="line">  optional MemoryDataParameter memory_data_param = 119;</div><div class="line">  optional MVNParameter mvn_param = 120;</div><div class="line">  optional ParameterParameter parameter_param = 145;</div><div class="line">  optional PoolingParameter pooling_param = 121;</div><div class="line">  optional PowerParameter power_param = 122;</div><div class="line">  optional PReLUParameter prelu_param = 131;</div><div class="line">  optional PythonParameter python_param = 130;</div><div class="line">  optional RecurrentParameter recurrent_param = 146;</div><div class="line">  optional ReductionParameter reduction_param = 136;</div><div class="line">  optional ReLUParameter relu_param = 123;</div><div class="line">  optional ReshapeParameter reshape_param = 133;</div><div class="line">  optional ROIPoolingParameter roi_pooling_param = 147;</div><div class="line">  optional ScaleParameter scale_param = 142;</div><div class="line">  optional SigmoidParameter sigmoid_param = 124;</div><div class="line">  optional SmoothL1LossParameter smooth_l1_loss_param = 148;</div><div class="line">  optional SoftmaxParameter softmax_param = 125;</div><div class="line">  optional SPPParameter spp_param = 132;</div><div class="line">  optional SliceParameter slice_param = 126;</div><div class="line">  optional TanHParameter tanh_param = 127;</div><div class="line">  optional ThresholdParameter threshold_param = 128;</div><div class="line">  optional TileParameter tile_param = 138;</div><div class="line">  optional WindowDataParameter window_data_param = 129;</div><div class="line">  optional MILDataParameter mil_data_param = 0x004d4944; //"MID"</div><div class="line">  optional MILParameter mil_param = 0x004d494c; //"MIL"</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 对数据层进行转换的参数</div><div class="line">message TransformationParameter &#123;</div><div class="line">  // 对data执行预处理，比如简单缩放,去均值。</div><div class="line">  optional float scale = 1 [default = 1];</div><div class="line">  // Specify if we want to randomly mirror data.//镜像</div><div class="line">  optional bool mirror = 2 [default = false];</div><div class="line">  // Specify if we would like to randomly crop an image.//随机裁剪</div><div class="line">  optional uint32 crop_size = 3 [default = 0];</div><div class="line">  // 指定均值文件或均值，二者不可兼有；在对应通道上减去此均值；</div><div class="line">  optional string mean_file = 4;</div><div class="line">  repeated float mean_value = 5;</div><div class="line">  // 强制转换图像为3通道彩色</div><div class="line">  optional bool force_color = 6 [default = false];</div><div class="line">  // 强制转换为灰度图</div><div class="line">  optional bool force_gray = 7 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Loss层参数</div><div class="line">message LossParameter &#123;</div><div class="line">  // 如果被指定，则忽略给定label的实例</div><div class="line">  optional int32 ignore_label = 1;</div><div class="line">  // 如何对loss层损失归一化，使其跨越"batches,spatial(H*W)"或其他维度。</div><div class="line">  // 目前仅仅在SoftmaxWithLoss层中实现。</div><div class="line">  // 归一化模式</div><div class="line">  enum NormalizationMode &#123;</div><div class="line">    // 基于batchSize×spatialDim归一化.所设定的忽略标签将不被忽略。</div><div class="line">    FULL = 0;</div><div class="line">    // 基于输出位置的总数量(batchSize×H×W)归一化，不包括被忽视的标签。</div><div class="line">    // 若未设置被忽视标签，则其行为与FULL相同。</div><div class="line">    VALID = 1;</div><div class="line">    // Divide by the batch size.基于batchSize进行归一化。</div><div class="line">    BATCH_SIZE = 2;</div><div class="line">    // Do not normalize the loss.不归一化损失</div><div class="line">    NONE = 3;</div><div class="line">  &#125;</div><div class="line">  optional NormalizationMode normalization = 3 [default = VALID];</div><div class="line">  // 旧版--新版如上所述。</div><div class="line">  // 若"normalization"被指定则忽略此参数；若未被指定，可设置下值为false</div><div class="line">  // 则基于batchSize归一化。</div><div class="line">  optional bool normalize = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Messages that store parameters used by individual layer types follow, in</div><div class="line">// alphabetical order.</div><div class="line"></div><div class="line">message AccuracyParameter &#123;</div><div class="line">  // When computing accuracy, count as correct by comparing the true label to</div><div class="line">  // the top k scoring classes.  By default, only compare to the top scoring</div><div class="line">  // class (i.e. argmax). //Topk正确率计算</div><div class="line">  optional uint32 top_k = 1 [default = 1];</div><div class="line"></div><div class="line">  // The "label" axis of the prediction blob, whose argmax corresponds to the</div><div class="line">  // predicted label -- may be negative to index from the end (e.g.,-1 for the</div><div class="line">  // last axis).  For example, if axis == 1 and the predictions are</div><div class="line">  // (N x C x H x W), the label blob is expected to contain N*H*W ground truth</div><div class="line">  // labels with integer values in &#123;0, 1, ..., C-1&#125;.</div><div class="line">  // 预测blob的"label"轴--其最大值才对应于预测标签--的索引有可能从负值开始。</div><div class="line">  // 即: predicted_labels=argmax(predictions blob,label_axis)</div><div class="line">  // 比如axis==1,其预测blob为(N x C x H x W), 而标签blob被期望包含(N×H×W)个</div><div class="line">  // 真实标签，且标签值为&#123;0,1,2...C-1&#125;。</div><div class="line">  optional int32 axis = 2 [default = 1];</div><div class="line"></div><div class="line">  // If specified, ignore instances with the given label.</div><div class="line">  // 如果指定，则忽略给定标签的对应实例</div><div class="line">  optional int32 ignore_label = 3;</div><div class="line">&#125;</div><div class="line">//输出最大化参数，对预测标签进行最大化</div><div class="line">message ArgMaxParameter &#123;</div><div class="line">  // If true produce pairs (argmax, maxval)</div><div class="line">  // 如果真，则产生(argmax,maxval)对</div><div class="line">  optional bool out_max_val = 1 [default = false];</div><div class="line">  optional uint32 top_k = 2 [default = 1];</div><div class="line">  // The axis along which to maximise -- may be negative to index from the</div><div class="line">  // end (e.g., -1 for the last axis).</div><div class="line">  // By default ArgMaxLayer maximizes over the flattened trailing dimensions</div><div class="line">  // for each index of the first / num dimension. ??</div><div class="line">  //</div><div class="line">  optional int32 axis = 3;</div><div class="line">&#125;</div><div class="line">//拼接参数</div><div class="line">message ConcatParameter &#123;</div><div class="line">  // The axis along which to concatenate -- may be negative to index from the</div><div class="line">  // end (e.g., -1 for the last axis).  Other axes must have the</div><div class="line">  // same dimension for all the bottom blobs.</div><div class="line">  // By default, ConcatLayer concatenates blobs along the "channels" axis (1).</div><div class="line">  optional int32 axis = 2 [default = 1];</div><div class="line"></div><div class="line">  // DEPRECATED: alias for "axis" -- does not support negative indexing.</div><div class="line">  optional uint32 concat_dim = 1 [default = 1];</div><div class="line">&#125;</div><div class="line">//BatchNormParameter参数，源于论文batchNorm</div><div class="line">message BatchNormParameter &#123;</div><div class="line">  // If false, accumulate global mean/variance values via a moving average. If</div><div class="line">  // true, use those accumulated values instead of computing mean/variance</div><div class="line">  // across the batch.</div><div class="line">  optional bool use_global_stats = 1;</div><div class="line">  // How much does the moving average decay each iteration?</div><div class="line">  optional float moving_average_fraction = 2 [default = .999];</div><div class="line">  // Small value to add to the variance estimate so that we don't divide by</div><div class="line">  // zero.</div><div class="line">  optional float eps = 3 [default = 1e-5];</div><div class="line">&#125;</div><div class="line">//偏置参数</div><div class="line">message BiasParameter &#123;</div><div class="line">  // The first axis of bottom[0] (the first input Blob) along which to apply</div><div class="line">  // bottom[1] (the second input Blob).  May be negative to index from the end</div><div class="line">  // (e.g., -1 for the last axis).</div><div class="line">  //</div><div class="line">  // For example, if bottom[0] is 4D with shape 100x3x40x60, the output</div><div class="line">  // top[0] will have the same shape, and bottom[1] may have any of the</div><div class="line">  // following shapes (for the given value of axis):</div><div class="line">  //    (axis == 0 == -4) 100; 100x3; 100x3x40; 100x3x40x60</div><div class="line">  //    (axis == 1 == -3)          3;     3x40;     3x40x60</div><div class="line">  //    (axis == 2 == -2)                   40;       40x60</div><div class="line">  //    (axis == 3 == -1)                                60</div><div class="line">  // Furthermore, bottom[1] may have the empty shape (regardless of the value of</div><div class="line">  // "axis") -- a scalar bias.</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // (num_axes is ignored unless just one bottom is given and the bias is</div><div class="line">  // a learned parameter of the layer.  Otherwise, num_axes is determined by the</div><div class="line">  // number of axes by the second bottom.)</div><div class="line">  // The number of axes of the input (bottom[0]) covered by the bias</div><div class="line">  // parameter, or -1 to cover all axes of bottom[0] starting from `axis`.</div><div class="line">  // Set num_axes := 0, to add a zero-axis Blob: a scalar.</div><div class="line">  optional int32 num_axes = 2 [default = 1];</div><div class="line"></div><div class="line">  // (filler is ignored unless just one bottom is given and the bias is</div><div class="line">  // a learned parameter of the layer.)</div><div class="line">  // The initialization for the learned bias parameter.</div><div class="line">  // Default is the zero (0) initialization, resulting in the BiasLayer</div><div class="line">  // initially performing the identity operation.</div><div class="line">  optional FillerParameter filler = 3;</div><div class="line">&#125;</div><div class="line">//对比度损失参数</div><div class="line">message ContrastiveLossParameter &#123;</div><div class="line">  // margin for dissimilar pair</div><div class="line">  optional float margin = 1 [default = 1.0];</div><div class="line">  // The first implementation of this cost did not exactly match the cost of</div><div class="line">  // Hadsell et al 2006 -- using (margin - d^2) instead of (margin - d)^2.</div><div class="line">  // legacy_version = false (the default) uses (margin - d)^2 as proposed in the</div><div class="line">  // Hadsell paper. New models should probably use this version.</div><div class="line">  // legacy_version = true uses (margin - d^2). This is kept to support /</div><div class="line">  // reproduce existing models and results</div><div class="line">  optional bool legacy_version = 2 [default = false];</div><div class="line">&#125;</div><div class="line">//卷积参数</div><div class="line">message ConvolutionParameter &#123;</div><div class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</div><div class="line">  optional bool bias_term = 2 [default = true]; // whether to have bias terms</div><div class="line"></div><div class="line">  // Pad, kernel size, and stride are all given as a single value for equal</div><div class="line">  // dimensions in all spatial dimensions, or once per spatial dimension.</div><div class="line">  repeated uint32 pad = 3; // The padding size; defaults to 0</div><div class="line">  repeated uint32 kernel_size = 4; // The kernel size</div><div class="line">  repeated uint32 stride = 6; // The stride; defaults to 1</div><div class="line">  // Factor used to dilate the kernel, (implicitly) zero-filling the resulting</div><div class="line">  // holes. (Kernel dilation is sometimes referred to by its use in the</div><div class="line">  // algorithme 脿 trous from Holschneider et al. 1987.)</div><div class="line">  repeated uint32 dilation = 18; // The dilation; defaults to 1</div><div class="line"></div><div class="line">  // For 2D convolution only, the *_h and *_w versions may also be used to</div><div class="line">  // specify both spatial dimensions.</div><div class="line">  optional uint32 pad_h = 9 [default = 0]; // The padding height (2D only)</div><div class="line">  optional uint32 pad_w = 10 [default = 0]; // The padding width (2D only)</div><div class="line">  optional uint32 kernel_h = 11; // The kernel height (2D only)</div><div class="line">  optional uint32 kernel_w = 12; // The kernel width (2D only)</div><div class="line">  optional uint32 stride_h = 13; // The stride height (2D only)</div><div class="line">  optional uint32 stride_w = 14; // The stride width (2D only)</div><div class="line"></div><div class="line">  optional uint32 group = 5 [default = 1]; // The group size for group conv</div><div class="line"></div><div class="line">  optional FillerParameter weight_filler = 7; // The filler for the weight</div><div class="line">  optional FillerParameter bias_filler = 8; // The filler for the bias</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0; //CPU</div><div class="line">    CAFFE = 1;   //GPU-CUDA</div><div class="line">    CUDNN = 2;   //GPU-CUDA-CUDNN</div><div class="line">  &#125;</div><div class="line">  optional Engine engine = 15 [default = DEFAULT];</div><div class="line"></div><div class="line">  // The axis to interpret as "channels" when performing convolution.</div><div class="line">  // Preceding dimensions are treated as independent inputs;</div><div class="line">  // succeeding dimensions are treated as "spatial".</div><div class="line">  // With (N, C, H, W) inputs, and axis == 1 (the default), we perform</div><div class="line">  // N independent 2D convolutions, sliding C-channel (or (C/g)-channels, for</div><div class="line">  // groups g&gt;1) filters across the spatial axes (H, W) of the input.</div><div class="line">  // With (N, C, D, H, W) inputs, and axis == 1, we perform</div><div class="line">  // N independent 3D convolutions, sliding (C/g)-channels</div><div class="line">  // filters across the spatial axes (D, H, W) of the input.</div><div class="line">  optional int32 axis = 16 [default = 1];</div><div class="line"></div><div class="line">  // Whether to force use of the general ND convolution, even if a specific</div><div class="line">  // implementation for blobs of the appropriate number of spatial dimensions</div><div class="line">  // is available. (Currently, there is only a 2D-specific convolution</div><div class="line">  // implementation; for input blobs with num_axes != 2, this option is</div><div class="line">  // ignored and the ND implementation will be used.)</div><div class="line">  optional bool force_nd_im2col = 17 [default = false];</div><div class="line">&#125;</div><div class="line">//裁剪参数</div><div class="line">message CropParameter &#123;</div><div class="line">  // To crop, elements of the first bottom are selected to fit the dimensions</div><div class="line">  // of the second, reference bottom. The crop is configured by</div><div class="line">  // - the crop `axis` to pick the dimensions for cropping</div><div class="line">  // - the crop `offset` to set the shift for all/each dimension</div><div class="line">  // to align the cropped bottom with the reference bottom.</div><div class="line">  // All dimensions up to but excluding `axis` are preserved, while</div><div class="line">  // the dimensions including and trailing `axis` are cropped.</div><div class="line">  // If only one `offset` is set, then all dimensions are offset by this amount.</div><div class="line">  // Otherwise, the number of offsets must equal the number of cropped axes to</div><div class="line">  // shift the crop in each dimension accordingly.</div><div class="line">  // Note: standard dimensions are N,C,H,W so the default is a spatial crop,</div><div class="line">  // and `axis` may be negative to index from the end (e.g., -1 for the last</div><div class="line">  // axis).</div><div class="line">  optional int32 axis = 1 [default = 2];</div><div class="line">  repeated uint32 offset = 2;</div><div class="line">&#125;</div><div class="line">//数据参数</div><div class="line">message DataParameter &#123;</div><div class="line">  enum DB &#123;</div><div class="line">    LEVELDB = 0;</div><div class="line">    LMDB = 1;</div><div class="line">  &#125;</div><div class="line">  // Specify the data source.</div><div class="line">  optional string source = 1;</div><div class="line">  // Specify the batch size.</div><div class="line">  optional uint32 batch_size = 4;</div><div class="line">  // The rand_skip variable is for the data layer to skip a few data points</div><div class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</div><div class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</div><div class="line">  // be larger than the number of keys in the database.</div><div class="line">  // DEPRECATED. Each solver accesses a different subset of the database.</div><div class="line">  optional uint32 rand_skip = 7 [default = 0];</div><div class="line">  optional DB backend = 8 [default = LEVELDB];</div><div class="line">  // DEPRECATED. See TransformationParameter. For data pre-processing, we can do</div><div class="line">  // simple scaling and subtracting the data mean, if provided. Note that the</div><div class="line">  // mean subtraction is always carried out before scaling.</div><div class="line">  optional float scale = 2 [default = 1];</div><div class="line">  optional string mean_file = 3;</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we would like to randomly</div><div class="line">  // crop an image.</div><div class="line">  optional uint32 crop_size = 5 [default = 0];</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we want to randomly mirror</div><div class="line">  // data.</div><div class="line">  optional bool mirror = 6 [default = false];</div><div class="line">  // Force the encoded image to have 3 color channels</div><div class="line">  optional bool force_encoded_color = 9 [default = false];</div><div class="line">  // Prefetch queue (Number of batches to prefetch to host memory, increase if</div><div class="line">  // data access bandwidth varies).</div><div class="line">  optional uint32 prefetch = 10 [default = 4];</div><div class="line">&#125;</div><div class="line">//DropoutParameter参数</div><div class="line">message DropoutParameter &#123;</div><div class="line">  optional float dropout_ratio = 1 [default = 0.5]; // dropout ratio</div><div class="line">  optional bool scale_train = 2 [default = true];  // scale train or test phase</div><div class="line">&#125;</div><div class="line"></div><div class="line">// DummyDataLayer fills any number of arbitrarily shaped blobs with random</div><div class="line">// (or constant) data generated by "Fillers" (see "message FillerParameter").</div><div class="line">message DummyDataParameter &#123;</div><div class="line">  // This layer produces N &gt;= 1 top blobs.  DummyDataParameter must specify 1 or N</div><div class="line">  // shape fields, and 0, 1 or N data_fillers.</div><div class="line">  //</div><div class="line">  // If 0 data_fillers are specified, ConstantFiller with a value of 0 is used.</div><div class="line">  // If 1 data_filler is specified, it is applied to all top blobs.  If N are</div><div class="line">  // specified, the ith is applied to the ith top blob.</div><div class="line">  repeated FillerParameter data_filler = 1;</div><div class="line">  repeated BlobShape shape = 6;</div><div class="line"></div><div class="line">  // 4D dimensions -- deprecated.  Use "shape" instead.</div><div class="line">  repeated uint32 num = 2;</div><div class="line">  repeated uint32 channels = 3;</div><div class="line">  repeated uint32 height = 4;</div><div class="line">  repeated uint32 width = 5;</div><div class="line">&#125;</div><div class="line"></div><div class="line">message EltwiseParameter &#123;</div><div class="line">  enum EltwiseOp &#123;</div><div class="line">    PROD = 0;</div><div class="line">    SUM = 1;</div><div class="line">    MAX = 2;</div><div class="line">  &#125;</div><div class="line">  optional EltwiseOp operation = 1 [default = SUM]; // element-wise operation</div><div class="line">  repeated float coeff = 2; // blob-wise coefficient for SUM operation</div><div class="line"></div><div class="line">  // Whether to use an asymptotically slower (for &gt;2 inputs) but stabler method</div><div class="line">  // of computing the gradient for the PROD operation. (No effect for SUM op.)</div><div class="line">  optional bool stable_prod_grad = 3 [default = true];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ELULayer</div><div class="line">message ELUParameter &#123;</div><div class="line">  // Described in:</div><div class="line">  // Clevert, D.-A., Unterthiner, T., &amp; Hochreiter, S. (2015). Fast and Accurate</div><div class="line">  // Deep Network Learning by Exponential Linear Units (ELUs). arXiv</div><div class="line">  optional float alpha = 1 [default = 1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by EmbedLayer</div><div class="line">message EmbedParameter &#123;</div><div class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</div><div class="line">  // The input is given as integers to be interpreted as one-hot</div><div class="line">  // vector indices with dimension num_input.  Hence num_input should be</div><div class="line">  // 1 greater than the maximum possible input value.</div><div class="line">  optional uint32 input_dim = 2;</div><div class="line"></div><div class="line">  optional bool bias_term = 3 [default = true]; // Whether to use a bias term</div><div class="line">  optional FillerParameter weight_filler = 4; // The filler for the weight</div><div class="line">  optional FillerParameter bias_filler = 5; // The filler for the bias</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ExpLayer</div><div class="line">message ExpParameter &#123;</div><div class="line">  // ExpLayer computes outputs y = base ^ (shift + scale * x), for base &gt; 0.</div><div class="line">  // Or if base is set to the default (-1), base is set to e,</div><div class="line">  // so y = exp(shift + scale * x).</div><div class="line">  optional float base = 1 [default = -1.0];</div><div class="line">  optional float scale = 2 [default = 1.0];</div><div class="line">  optional float shift = 3 [default = 0.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line">/// Message that stores parameters used by FlattenLayer</div><div class="line">message FlattenParameter &#123;</div><div class="line">  // The first axis to flatten: all preceding axes are retained in the output.</div><div class="line">  // May be negative to index from the end (e.g., -1 for the last axis).</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // The last axis to flatten: all following axes are retained in the output.</div><div class="line">  // May be negative to index from the end (e.g., the default -1 for the last</div><div class="line">  // axis).</div><div class="line">  optional int32 end_axis = 2 [default = -1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by HDF5DataLayer</div><div class="line">message HDF5DataParameter &#123;</div><div class="line">  // Specify the data source.</div><div class="line">  optional string source = 1;</div><div class="line">  // Specify the batch size.</div><div class="line">  optional uint32 batch_size = 2;</div><div class="line"></div><div class="line">  // Specify whether to shuffle the data.</div><div class="line">  // If shuffle == true, the ordering of the HDF5 files is shuffled,</div><div class="line">  // and the ordering of data within any given HDF5 file is shuffled,</div><div class="line">  // but data between different files are not interleaved; all of a file's</div><div class="line">  // data are output (in a random order) before moving onto another file.</div><div class="line">  optional bool shuffle = 3 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">message HDF5OutputParameter &#123;</div><div class="line">  optional string file_name = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">message HingeLossParameter &#123;</div><div class="line">  enum Norm &#123;</div><div class="line">    L1 = 1;</div><div class="line">    L2 = 2;</div><div class="line">  &#125;</div><div class="line">  // Specify the Norm to use L1 or L2</div><div class="line">  optional Norm norm = 1 [default = L1];</div><div class="line">&#125;</div><div class="line">//数据集参数</div><div class="line">message ImageDataParameter &#123;</div><div class="line">  // 指定数据源文件</div><div class="line">  optional string source = 1;</div><div class="line">  // 指定批量大小batchSize</div><div class="line">  optional uint32 batch_size = 4 [default = 1];</div><div class="line">  // The rand_skip variable is for the data layer to skip a few data points</div><div class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</div><div class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</div><div class="line">  // be larger than the number of keys in the database.</div><div class="line">  // 随机跳过rand_skip * rand(0,1)个样本，以使得SGD从不同状态点启动</div><div class="line">  optional uint32 rand_skip = 7 [default = 0];</div><div class="line">  // Whether or not ImageLayer should shuffle the list of files at every epoch.是否在每个回合都混排图片，默认否</div><div class="line">  optional bool shuffle = 8 [default = false];</div><div class="line">  // It will also resize images if new_height or new_width are not zero.</div><div class="line">  // 若以下2个值不为0，则将图片缩放为下面的形状</div><div class="line">  optional uint32 new_height = 9 [default = 0];</div><div class="line">  optional uint32 new_width = 10 [default = 0];</div><div class="line">  // Specify if the images are color or gray指明是彩色还是灰度图</div><div class="line">  optional bool is_color = 11 [default = true];</div><div class="line">  // DEPRECATED. See TransformationParameter. For data pre-processing, we can do</div><div class="line">  // simple scaling and subtracting the data mean, if provided. Note that the</div><div class="line">  // mean subtraction is always carried out before scaling.</div><div class="line">  // 旧版--图片预处理参数，新版用TransformationParameter</div><div class="line">  optional float scale = 2 [default = 1];</div><div class="line">  optional string mean_file = 3;</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we would like to randomly</div><div class="line">  // crop an image.</div><div class="line">  optional uint32 crop_size = 5 [default = 0];</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we want to randomly mirror</div><div class="line">  // data.</div><div class="line">  optional bool mirror = 6 [default = false];</div><div class="line">  optional string root_folder = 12 [default = ""];</div><div class="line">&#125;</div><div class="line">//信息增益损失参数</div><div class="line">message InfogainLossParameter &#123;</div><div class="line">  // Specify the infogain matrix source.</div><div class="line">  optional string source = 1;</div><div class="line">&#125;</div><div class="line">//内积参数</div><div class="line">message InnerProductParameter &#123;</div><div class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</div><div class="line">  optional bool bias_term = 2 [default = true]; // whether to have bias terms</div><div class="line">  optional FillerParameter weight_filler = 3; // The filler for the weight</div><div class="line">  optional FillerParameter bias_filler = 4; // The filler for the bias</div><div class="line"></div><div class="line">  // The first axis to be lumped into a single inner product computation;</div><div class="line">  // all preceding axes are retained in the output.</div><div class="line">  // May be negative to index from the end (e.g., -1 for the last axis).</div><div class="line">  optional int32 axis = 5 [default = 1];</div><div class="line">  // Specify whether to transpose the weight matrix or not.</div><div class="line">  // If transpose == true, any operations will be performed on the transpose</div><div class="line">  // of the weight matrix. The weight matrix itself is not going to be transposed</div><div class="line">  // but rather the transfer flag of operations will be toggled accordingly.</div><div class="line">  optional bool transpose = 6 [default = false];</div><div class="line">&#125;</div><div class="line">//输入参数</div><div class="line">message InputParameter &#123;</div><div class="line">  // This layer produces N &gt;= 1 top blob(s) to be assigned manually.</div><div class="line">  // Define N shapes to set a shape for each top.</div><div class="line">  // Define 1 shape to set the same shape for every top.</div><div class="line">  // Define no shape to defer to reshaping manually.</div><div class="line">  // 此层管理输入(top)blobs，当输入blob个数N≥1，可使其自动分配。</div><div class="line">  // 设定N个shapes为N个输入blob；设定1个shape使得全部输入blob形状相同；</div><div class="line">  // 不设定，可手动调整。</div><div class="line">  // 可查看.\models\bvlc_reference_caffenet\deploy.prototxt中指定1个shape</div><div class="line">  repeated BlobShape shape = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// LogLayer的参数</div><div class="line">message LogParameter &#123;</div><div class="line">  // LogLayer computes outputs y = log_base(shift + scale * x), for base &gt; 0.</div><div class="line">  // Or if base is set to the default (-1), base is set to e,</div><div class="line">  // so y = ln(shift + scale * x) = log_e(shift + scale * x)</div><div class="line">  optional float base = 1 [default = -1.0];</div><div class="line">  optional float scale = 2 [default = 1.0];</div><div class="line">  optional float shift = 3 [default = 0.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// LRNLayer层参数</div><div class="line">message LRNParameter &#123;</div><div class="line">  optional uint32 local_size = 1 [default = 5];</div><div class="line">  optional float alpha = 2 [default = 1.];</div><div class="line">  optional float beta = 3 [default = 0.75];</div><div class="line">  enum NormRegion &#123;</div><div class="line">    ACROSS_CHANNELS = 0;</div><div class="line">    WITHIN_CHANNEL = 1;</div><div class="line">  &#125;</div><div class="line">  optional NormRegion norm_region = 4 [default = ACROSS_CHANNELS];</div><div class="line">  optional float k = 5 [default = 1.];</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  optional Engine engine = 6 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">//数据内存占用参数</div><div class="line">message MemoryDataParameter &#123;</div><div class="line">  optional uint32 batch_size = 1;</div><div class="line">  optional uint32 channels = 2;</div><div class="line">  optional uint32 height = 3;</div><div class="line">  optional uint32 width = 4;</div><div class="line">&#125;</div><div class="line">//MVN参数&#123;均值，方差，跨通道&#125;(mean-varance-normalization)</div><div class="line">message MVNParameter &#123;</div><div class="line">  // This parameter can be set to false to normalize mean only</div><div class="line">  // 设定为false时仅归一化均值，否则包括方差</div><div class="line">  optional bool normalize_variance = 1 [default = true];</div><div class="line"></div><div class="line">  // This parameter can be set to true to perform DNN-like MVN</div><div class="line">  // 执行跨通道归一化,类似于DNN的MVN；默认否，只执行Spatial内归一化。</div><div class="line">  optional bool across_channels = 2 [default = false];</div><div class="line"></div><div class="line">  // Epsilon for not dividing by zero while normalizing variance</div><div class="line">  // 防止除0的极小数</div><div class="line">  optional float eps = 3 [default = 1e-9];</div><div class="line">&#125;</div><div class="line"></div><div class="line">//？？</div><div class="line">message ParameterParameter &#123;</div><div class="line">  optional BlobShape shape = 1;</div><div class="line">&#125;</div><div class="line">//池化层参数</div><div class="line">message PoolingParameter &#123;</div><div class="line">  enum PoolMethod &#123;</div><div class="line">    MAX = 0;</div><div class="line">    AVE = 1;</div><div class="line">    STOCHASTIC = 2;</div><div class="line">  &#125;</div><div class="line">  optional PoolMethod pool = 1 [default = MAX]; // The pooling method</div><div class="line">  // Pad, kernel size, and stride are all given as a single value for equal</div><div class="line">  // dimensions in height and width or as Y, X pairs.</div><div class="line">  optional uint32 pad = 4 [default = 0]; // The padding size (equal in Y, X)</div><div class="line">  optional uint32 pad_h = 9 [default = 0]; // The padding height</div><div class="line">  optional uint32 pad_w = 10 [default = 0]; // The padding width</div><div class="line">  optional uint32 kernel_size = 2; // The kernel size (square)</div><div class="line">  optional uint32 kernel_h = 5; // The kernel height</div><div class="line">  optional uint32 kernel_w = 6; // The kernel width</div><div class="line">  optional uint32 stride = 3 [default = 1]; // The stride (equal in Y, X)</div><div class="line">  optional uint32 stride_h = 7; // The stride height</div><div class="line">  optional uint32 stride_w = 8; // The stride width</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  optional Engine engine = 11 [default = DEFAULT];</div><div class="line">  // If global_pooling then it will pool over the size of the bottom by doing</div><div class="line">  // kernel_h = bottom-&gt;height and kernel_w = bottom-&gt;width</div><div class="line">  optional bool global_pooling = 12 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">message PowerParameter &#123;</div><div class="line">  // PowerLayer computes outputs y = (shift + scale * x) ^ power.</div><div class="line">  optional float power = 1 [default = 1.0];</div><div class="line">  optional float scale = 2 [default = 1.0];</div><div class="line">  optional float shift = 3 [default = 0.0];</div><div class="line">&#125;</div><div class="line">//Python参数</div><div class="line">message PythonParameter &#123;</div><div class="line">  optional string module = 1;</div><div class="line">  optional string layer = 2;</div><div class="line">  // This value is set to the attribute `param_str` of the `PythonLayer` object</div><div class="line">  // in Python before calling the `setup()` method. This could be a number,</div><div class="line">  // string, dictionary in Python dict format, JSON, etc. You may parse this</div><div class="line">  // string in `setup` method and use it in `forward` and `backward`.</div><div class="line">  optional string param_str = 3 [default = ''];</div><div class="line">  // Whether this PythonLayer is shared among worker solvers during data parallelism.</div><div class="line">  // If true, each worker solver sequentially run forward from this layer.</div><div class="line">  // This value should be set true if you are using it as a data layer.</div><div class="line">  optional bool share_in_parallel = 4 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// RecurrentLayer参数</div><div class="line">message RecurrentParameter &#123;</div><div class="line">  // 输出表示的维度必须是非0的</div><div class="line">  optional uint32 num_output = 1 [default = 0];</div><div class="line"></div><div class="line">  optional FillerParameter weight_filler = 2; //weight权值参数</div><div class="line">  optional FillerParameter bias_filler = 3;   //bias偏置参数</div><div class="line"></div><div class="line">  // Whether to enable displaying debug_info in the unrolled recurrent net.</div><div class="line">  // 在展开RCNN时是否打印deuginfo</div><div class="line">  optional bool debug_info = 4 [default = false];</div><div class="line"></div><div class="line">  // Whether to add as additional inputs (bottoms) the initial hidden state</div><div class="line">  // blobs, and add as additional outputs (tops) the final timestep hidden state</div><div class="line">  // blobs.  The number of additional bottom/top blobs required depends on the</div><div class="line">  // recurrent architecture -- e.g., 1 for RNNs, 2 for LSTMs.</div><div class="line">  // 是否添加初始化的隐藏blobs作为额外输入(bottoms)，以及添加最终的timestep隐</div><div class="line">  // 藏blobs作为额外输出（tops）。</div><div class="line">  optional bool expose_hidden = 5 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// ReductionLayer参数</div><div class="line">message ReductionParameter &#123;</div><div class="line">  enum ReductionOp &#123;</div><div class="line">    SUM = 1;</div><div class="line">    ASUM = 2;</div><div class="line">    SUMSQ = 3;</div><div class="line">    MEAN = 4;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  optional ReductionOp operation = 1 [default = SUM]; // reduction operation</div><div class="line"></div><div class="line">  // The first axis to reduce to a scalar -- may be negative to index from the</div><div class="line">  // end (e.g., -1 for the last axis).</div><div class="line">  // (Currently, only reduction along ALL "tail" axes is supported; reduction</div><div class="line">  // of axis M through N, where N &lt; num_axes - 1, is unsupported.)</div><div class="line">  // Suppose we have an n-axis bottom Blob with shape:</div><div class="line">  //     (d0, d1, d2, ..., d(m-1), dm, d(m+1), ..., d(n-1)).</div><div class="line">  // If axis == m, the output Blob will have shape</div><div class="line">  //     (d0, d1, d2, ..., d(m-1)),</div><div class="line">  // and the ReductionOp operation is performed (d0 * d1 * d2 * ... * d(m-1))</div><div class="line">  // times, each including (dm * d(m+1) * ... * d(n-1)) individual data.</div><div class="line">  // If axis == 0 (the default), the output Blob always has the empty shape</div><div class="line">  // (count 1), performing reduction across the entire input --</div><div class="line">  // often useful for creating new loss functions.</div><div class="line">  optional int32 axis = 2 [default = 0];</div><div class="line"></div><div class="line">  optional float coeff = 3 [default = 1.0]; // coefficient for output</div><div class="line">&#125;</div><div class="line"></div><div class="line">// ReLULayer参数</div><div class="line">message ReLUParameter &#123;</div><div class="line">  // 允许非0斜率可以加速优化:</div><div class="line">  // Maas, A. L., Hannun, A. Y., &amp; Ng, A. Y. (2013). Rectifier nonlinearities</div><div class="line">  // improve neural network acoustic models. In ICML Workshop on Deep Learning</div><div class="line">  // for Audio, Speech, and Language Processing.</div><div class="line">  optional float negative_slope = 1 [default = 0];</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  optional Engine engine = 2 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">message ReshapeParameter &#123;</div><div class="line">  // Specify the output dimensions. If some of the dimensions are set to 0,</div><div class="line">  // the corresponding dimension from the bottom layer is used (unchanged).</div><div class="line">  // Exactly one dimension may be set to -1, in which case its value is</div><div class="line">  // inferred from the count of the bottom blob and the remaining dimensions.</div><div class="line">  // For example, suppose we want to reshape a 2D blob "input" with shape 2 x 8:</div><div class="line">  //</div><div class="line">  //   layer &#123;</div><div class="line">  //     type: "Reshape" bottom: "input" top: "output"</div><div class="line">  //     reshape_param &#123; ... &#125;</div><div class="line">  //   &#125;</div><div class="line">  //</div><div class="line">  // If "input" is 2D with shape 2 x 8, then the following reshape_param</div><div class="line">  // specifications are all equivalent, producing a 3D blob "output" with shape</div><div class="line">  // 2 x 2 x 4:</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  2  dim: 2  dim:  4 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim: 2  dim:  4 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim: 2  dim: -1 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim:-1  dim:  4 &#125; &#125;</div><div class="line">  //</div><div class="line">  optional BlobShape shape = 1;</div><div class="line"></div><div class="line">  // axis and num_axes control the portion of the bottom blob's shape that are</div><div class="line">  // replaced by (included in) the reshape. By default (axis == 0 and</div><div class="line">  // num_axes == -1), the entire bottom blob shape is included in the reshape,</div><div class="line">  // and hence the shape field must specify the entire output shape.</div><div class="line">  //</div><div class="line">  // axis may be non-zero to retain some portion of the beginning of the input</div><div class="line">  // shape (and may be negative to index from the end; e.g., -1 to begin the</div><div class="line">  // reshape after the last axis, including nothing in the reshape,</div><div class="line">  // -2 to include only the last axis, etc.).</div><div class="line">  //</div><div class="line">  // For example, suppose "input" is a 2D blob with shape 2 x 8.</div><div class="line">  // Then the following ReshapeLayer specifications are all equivalent,</div><div class="line">  // producing a blob "output" with shape 2 x 2 x 4:</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 2  dim: 4 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 4 &#125; axis:  1 &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 4 &#125; axis: -3 &#125;</div><div class="line">  //</div><div class="line">  // num_axes specifies the extent of the reshape.</div><div class="line">  // If num_axes &gt;= 0 (and axis &gt;= 0), the reshape will be performed only on</div><div class="line">  // input axes in the range [axis, axis+num_axes].</div><div class="line">  // num_axes may also be -1, the default, to include all remaining axes</div><div class="line">  // (starting from axis).</div><div class="line">  //</div><div class="line">  // For example, suppose "input" is a 2D blob with shape 2 x 8.</div><div class="line">  // Then the following ReshapeLayer specifications are equivalent,</div><div class="line">  // producing a blob "output" with shape 1 x 2 x 8.</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  1  dim: 2  dim:  8 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  1  dim: 2  &#125;  num_axes: 1 &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  1  &#125;  num_axes: 0 &#125;</div><div class="line">  //</div><div class="line">  // On the other hand, these would produce output blob shape 2 x 1 x 8:</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 1  dim: 8  &#125;  &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 1 &#125;  axis: 1  num_axes: 0 &#125;</div><div class="line">  //</div><div class="line">  optional int32 axis = 2 [default = 0];</div><div class="line">  optional int32 num_axes = 3 [default = -1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// ROIPoolingLayer参数</div><div class="line">message ROIPoolingParameter &#123;</div><div class="line">  // Pad, kernel size, and stride are all given as a single value for equal</div><div class="line">  // dimensions in height and width or as Y, X pairs.</div><div class="line">  optional uint32 pooled_h = 1 [default = 0]; // The pooled output height</div><div class="line">  optional uint32 pooled_w = 2 [default = 0]; // The pooled output width</div><div class="line">  // Multiplicative spatial scale factor to translate ROI coords from their</div><div class="line">  // input scale to the scale used when pooling</div><div class="line">  optional float spatial_scale = 3 [default = 1];</div><div class="line">&#125;</div><div class="line">//ScaleParameter参数</div><div class="line">message ScaleParameter &#123;</div><div class="line">  // The first axis of bottom[0] (the first input Blob) along which to apply</div><div class="line">  // bottom[1] (the second input Blob).  May be negative to index from the end</div><div class="line">  // (e.g., -1 for the last axis).</div><div class="line">  // ？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？</div><div class="line">  // 第一个输入Blob的首axis，被应用到第二个输入Blob。但第2个Blob的形状可能不同</div><div class="line">  // For example, if bottom[0] is 4D with shape 100x3x40x60, the output</div><div class="line">  // top[0] will have the same shape, and bottom[1] may have any of the</div><div class="line">  // following shapes (for the given value of axis):</div><div class="line">  //    (axis == 0 == -4) 100; 100x3; 100x3x40; 100x3x40x60</div><div class="line">  //    (axis == 1 == -3)          3;     3x40;     3x40x60</div><div class="line">  //    (axis == 2 == -2)                   40;       40x60</div><div class="line">  //    (axis == 3 == -1)                                60</div><div class="line">  // Furthermore,bottom[1]may have the empty shape (regardless of the value of</div><div class="line">  // "axis") -- a scalar multiplier.</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // (num_axes is ignored unless just one bottom is given and the scale is</div><div class="line">  // a learned parameter of the layer.  Otherwise, num_axes is determined by the</div><div class="line">  // number of axes by the second bottom.)</div><div class="line">  // The number of axes of the input (bottom[0]) covered by the scale</div><div class="line">  // parameter, or -1 to cover all axes of bottom[0] starting from `axis`.</div><div class="line">  // Set num_axes := 0, to multiply with a zero-axis Blob: a scalar.</div><div class="line">  optional int32 num_axes = 2 [default = 1];</div><div class="line"></div><div class="line">  // (filler is ignored unless just one bottom is given and the scale is</div><div class="line">  // a learned parameter of the layer.)</div><div class="line">  // The initialization for the learned scale parameter.</div><div class="line">  // Default is the unit (1) initialization, resulting in the ScaleLayer</div><div class="line">  // initially performing the identity operation.</div><div class="line">  optional FillerParameter filler = 3;</div><div class="line"></div><div class="line">  // Whether to also learn a bias (equivalent to a ScaleLayer+BiasLayer, but</div><div class="line">  // may be more efficient).  Initialized with bias_filler (defaults to 0).</div><div class="line">  optional bool bias_term = 4 [default = false];</div><div class="line">  optional FillerParameter bias_filler = 5;</div><div class="line">&#125;</div><div class="line">//SigmoidParameter参数</div><div class="line">message SigmoidParameter &#123;</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  optional Engine engine = 1 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line">//SliceParameter参数</div><div class="line">message SliceParameter &#123;</div><div class="line">  // The axis along which to slice -- may be negative to index from the end</div><div class="line">  // (e.g., -1 for the last axis).</div><div class="line">  // By default, SliceLayer concatenates blobs along the "channels" axis (1).</div><div class="line">  optional int32 axis = 3 [default = 1];</div><div class="line">  repeated uint32 slice_point = 2;</div><div class="line"></div><div class="line">  // DEPRECATED: alias for "axis" -- does not support negative indexing.</div><div class="line">  optional uint32 slice_dim = 1 [default = 1];</div><div class="line">&#125;</div><div class="line">//SmoothL1LossParameter参数</div><div class="line">message SmoothL1LossParameter &#123;</div><div class="line">  // SmoothL1Loss(x) =</div><div class="line">  //   0.5 * (sigma * x) ** 2    -- if x &lt; 1.0 / sigma / sigma</div><div class="line">  //   |x| - 0.5 / sigma / sigma -- otherwise</div><div class="line">  optional float sigma = 1 [default = 1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">//SoftmaxLayer, SoftmaxWithLossLayer的参数</div><div class="line">message SoftmaxParameter &#123;</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  optional Engine engine = 1 [default = DEFAULT];</div><div class="line"></div><div class="line">  // The axis along which to perform the softmax -- may be negative to index</div><div class="line">  // from the end (e.g., -1 for the last axis).</div><div class="line">  // Any other axes will be evaluated as independent softmaxes.</div><div class="line">  // 沿着哪一个轴运用softmax，该轴上必须是相互独立的分量。</div><div class="line">  // eg.预测时对类标签运用，计算损失时对每个类的对数损失运用。</div><div class="line">  optional int32 axis = 2 [default = 1];</div><div class="line">&#125;</div><div class="line">//TanHParameter参数</div><div class="line">message TanHParameter &#123;</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  optional Engine engine = 1 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// TileLayer参数</div><div class="line">message TileParameter &#123;</div><div class="line">  // The index of the axis to tile.</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // The number of copies (tiles) of the blob to output.</div><div class="line">  optional int32 tiles = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// ThresholdLayer参数</div><div class="line">message ThresholdParameter &#123;</div><div class="line">  optional float threshold = 1 [default = 0]; // Strictly positive values</div><div class="line">&#125;</div><div class="line"></div><div class="line">// MILLayer参数</div><div class="line">message MILParameter &#123;</div><div class="line">  enum MILType &#123;</div><div class="line">    MAX = 0;</div><div class="line">    NOR = 1;</div><div class="line">  &#125;</div><div class="line">  optional MILType type = 1 [default = MAX]; // The MIL method</div><div class="line">&#125;</div><div class="line"></div><div class="line">//窗口数据参数：专用于目标检测或分割</div><div class="line">message WindowDataParameter &#123;</div><div class="line">  // Specify the data source.指定数据源</div><div class="line">  optional string source = 1;</div><div class="line">  // 数据预处理：尺度缩放，去均值等。去均值应在缩放前执行。</div><div class="line">  optional float scale = 2 [default = 1];</div><div class="line">  optional string mean_file = 3;</div><div class="line">  // 指定批处理的数据量</div><div class="line">  optional uint32 batch_size = 4;</div><div class="line">  // 是否随机裁剪</div><div class="line">  optional uint32 crop_size = 5 [default = 0];</div><div class="line">  // 是否镜像变换</div><div class="line">  optional bool mirror = 6 [default = false];</div><div class="line">  // Foreground (object) overlap threshold 前景目标重合阈值</div><div class="line">  optional float fg_threshold = 7 [default = 0.5];</div><div class="line">  // Background (non-object) overlap threshold背景重合阈值</div><div class="line">  optional float bg_threshold = 8 [default = 0.5];</div><div class="line">  // Fraction of batch that should be foreground objects</div><div class="line">  // 前景目标在batch中的比例</div><div class="line">  optional float fg_fraction = 9 [default = 0.25];</div><div class="line">  // Amount of contextual padding to add around a window</div><div class="line">  // (used only by the window_data_layer)</div><div class="line">  // 窗口周边需要添加的上下文padding</div><div class="line">  optional uint32 context_pad = 10 [default = 0];</div><div class="line">  // Mode for cropping out a detection window</div><div class="line">  // warp: cropped window is warped to a fixed size and aspect ratio</div><div class="line">  // square: the tightest square around the window is cropped</div><div class="line">  // mode:裁剪出一个检测窗口的模式</div><div class="line">  // warp:裁剪窗口被扭曲为某个固定尺寸和形状</div><div class="line">  // square:裁剪窗口周边最紧？的方框</div><div class="line">  optional string crop_mode = 11 [default = "warp"];</div><div class="line">  // cache_images: will load all images in memory for faster access</div><div class="line">  //将全部图像(裁剪得到的小图像)放入内存以便快速存取</div><div class="line">  optional bool cache_images = 12 [default = false];</div><div class="line">  // append root_folder to locate images</div><div class="line">  // 添加根文件夹以定位文件</div><div class="line">  optional string root_folder = 13 [default = ""];</div><div class="line">&#125;</div><div class="line">//MILDataParameter参数</div><div class="line">message MILDataParameter &#123;</div><div class="line">  // Specify the data source.</div><div class="line">  optional string source = 1;</div><div class="line"></div><div class="line">  // Number of scales for each image</div><div class="line">  optional uint32 num_scales = 2 [default = 1];</div><div class="line"></div><div class="line">  // Side length ratio between neighbouring scales</div><div class="line">  optional float scale_factor = 6 [default = 1];</div><div class="line"></div><div class="line">  // Number of channels in the image</div><div class="line">  optional uint32 channels = 4 [default = 3];</div><div class="line"></div><div class="line">  // Specify the number of images per batch</div><div class="line">  optional uint32 images_per_batch = 3;</div><div class="line">  // Specify the number of classes</div><div class="line">  optional uint32 n_classes = 5;</div><div class="line">  // specify the box_dir and label_dir</div><div class="line">  optional string label_file = 7;</div><div class="line"></div><div class="line">  // Root directory which contains all the images</div><div class="line">  optional string root_dir = 11;</div><div class="line">  // Extention for the file</div><div class="line">  optional string ext = 12;</div><div class="line"></div><div class="line">  // To randomize or not</div><div class="line">  optional bool randomize = 13 [default = true];</div><div class="line">&#125;</div><div class="line"></div><div class="line">//SPP参数,源于论文SPPNet</div><div class="line">message SPPParameter &#123;</div><div class="line">  enum PoolMethod &#123;</div><div class="line">    MAX = 0;</div><div class="line">    AVE = 1;</div><div class="line">    STOCHASTIC = 2;</div><div class="line">  &#125; //池化方法，获得金字塔的方法，最大/平均/随机</div><div class="line">  optional uint32 pyramid_height = 1; //金字塔高度</div><div class="line">  optional PoolMethod pool = 2 [default = MAX]; // The pooling method</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  optional Engine engine = 6 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// DEPRECATED: use LayerParameter.</div><div class="line">// 旧版：使用层参数。 V1可能是第一版version1的意思</div><div class="line">message V1LayerParameter &#123;</div><div class="line">  repeated string bottom = 2;  //输入</div><div class="line">  repeated string top = 3;     //输出</div><div class="line">  optional string name = 4;    //层名称</div><div class="line">  repeated NetStateRule include = 32; //运行时状态：包含</div><div class="line">  repeated NetStateRule exclude = 33; //运行时状态：不包含</div><div class="line">  enum LayerType &#123;             //层类型</div><div class="line">    NONE = 0;</div><div class="line">    ABSVAL = 35;</div><div class="line">    ACCURACY = 1;</div><div class="line">    ARGMAX = 30;</div><div class="line">    BNLL = 2;</div><div class="line">    CONCAT = 3;</div><div class="line">    CONTRASTIVE_LOSS = 37;</div><div class="line">    CONVOLUTION = 4;</div><div class="line">    DATA = 5;</div><div class="line">    DECONVOLUTION = 39;</div><div class="line">    DROPOUT = 6;</div><div class="line">    DUMMY_DATA = 32;</div><div class="line">    EUCLIDEAN_LOSS = 7;</div><div class="line">    ELTWISE = 25;</div><div class="line">    EXP = 38;</div><div class="line">    FLATTEN = 8;</div><div class="line">    HDF5_DATA = 9;</div><div class="line">    HDF5_OUTPUT = 10;</div><div class="line">    HINGE_LOSS = 28;</div><div class="line">    IM2COL = 11;</div><div class="line">    IMAGE_DATA = 12;</div><div class="line">    INFOGAIN_LOSS = 13;</div><div class="line">    INNER_PRODUCT = 14;</div><div class="line">    LRN = 15;</div><div class="line">    MEMORY_DATA = 29;</div><div class="line">    MULTINOMIAL_LOGISTIC_LOSS = 16;</div><div class="line">    MVN = 34;</div><div class="line">    POOLING = 17;</div><div class="line">    POWER = 26;</div><div class="line">    RELU = 18;</div><div class="line">    SIGMOID = 19;</div><div class="line">    SIGMOID_CROSS_ENTROPY_LOSS = 27;</div><div class="line">    SILENCE = 36;</div><div class="line">    SOFTMAX = 20;</div><div class="line">    SOFTMAX_LOSS = 21;</div><div class="line">    SPLIT = 22;</div><div class="line">    SLICE = 33;</div><div class="line">    TANH = 23;</div><div class="line">    WINDOW_DATA = 24;</div><div class="line">    THRESHOLD = 31;</div><div class="line">  &#125;</div><div class="line">  optional LayerType type = 5;</div><div class="line">  repeated BlobProto blobs = 6;</div><div class="line">  repeated string param = 1001;</div><div class="line">  repeated DimCheckMode blob_share_mode = 1002;</div><div class="line">  enum DimCheckMode &#123;</div><div class="line">    STRICT = 0;</div><div class="line">    PERMISSIVE = 1;</div><div class="line">  &#125;</div><div class="line">  repeated float blobs_lr = 7;</div><div class="line">  repeated float weight_decay = 8;</div><div class="line">  repeated float loss_weight = 35;</div><div class="line">  optional AccuracyParameter accuracy_param = 27;</div><div class="line">  optional ArgMaxParameter argmax_param = 23;</div><div class="line">  optional ConcatParameter concat_param = 9;</div><div class="line">  optional ContrastiveLossParameter contrastive_loss_param = 40;</div><div class="line">  optional ConvolutionParameter convolution_param = 10;</div><div class="line">  optional DataParameter data_param = 11;</div><div class="line">  optional DropoutParameter dropout_param = 12;</div><div class="line">  optional DummyDataParameter dummy_data_param = 26;</div><div class="line">  optional EltwiseParameter eltwise_param = 24;</div><div class="line">  optional ExpParameter exp_param = 41;</div><div class="line">  optional HDF5DataParameter hdf5_data_param = 13;</div><div class="line">  optional HDF5OutputParameter hdf5_output_param = 14;</div><div class="line">  optional HingeLossParameter hinge_loss_param = 29;</div><div class="line">  optional ImageDataParameter image_data_param = 15;</div><div class="line">  optional InfogainLossParameter infogain_loss_param = 16;</div><div class="line">  optional InnerProductParameter inner_product_param = 17;</div><div class="line">  optional LRNParameter lrn_param = 18;</div><div class="line">  optional MemoryDataParameter memory_data_param = 22;</div><div class="line">  optional MVNParameter mvn_param = 34;</div><div class="line">  optional PoolingParameter pooling_param = 19;</div><div class="line">  optional PowerParameter power_param = 21;</div><div class="line">  optional ReLUParameter relu_param = 30;</div><div class="line">  optional SigmoidParameter sigmoid_param = 38;</div><div class="line">  optional SoftmaxParameter softmax_param = 39;</div><div class="line">  optional SliceParameter slice_param = 31;</div><div class="line">  optional TanHParameter tanh_param = 37;</div><div class="line">  optional ThresholdParameter threshold_param = 25;</div><div class="line">  optional WindowDataParameter window_data_param = 20;</div><div class="line">  optional TransformationParameter transform_param = 36;</div><div class="line">  optional LossParameter loss_param = 42;</div><div class="line">  optional V0LayerParameter layer = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// DEPRECATED: V0LayerParameter is the old way of specifying layer parameters</div><div class="line">// in Caffe.  We keep this message type around for legacy support.</div><div class="line">// 旧版本：V0LayerParameter version-0版</div><div class="line">message V0LayerParameter &#123;</div><div class="line">  optional string name = 1; // the layer name</div><div class="line">  optional string type = 2; // the string to specify the layer type</div><div class="line"></div><div class="line">  // Parameters to specify layers with inner products.</div><div class="line">  optional uint32 num_output = 3; // The number of outputs for the layer</div><div class="line">  optional bool biasterm = 4 [default = true]; // whether to have bias terms</div><div class="line">  optional FillerParameter weight_filler = 5; // The filler for the weight</div><div class="line">  optional FillerParameter bias_filler = 6; // The filler for the bias</div><div class="line"></div><div class="line">  optional uint32 pad = 7 [default = 0]; // The padding size</div><div class="line">  optional uint32 kernelsize = 8; // The kernel size</div><div class="line">  optional uint32 group = 9 [default = 1]; // The group size for group conv</div><div class="line">  optional uint32 stride = 10 [default = 1]; // The stride</div><div class="line">  enum PoolMethod &#123;</div><div class="line">    MAX = 0;</div><div class="line">    AVE = 1;</div><div class="line">    STOCHASTIC = 2;</div><div class="line">  &#125;</div><div class="line">  optional PoolMethod pool = 11 [default = MAX]; // The pooling method</div><div class="line">  optional float dropout_ratio = 12 [default = 0.5]; // dropout ratio</div><div class="line"></div><div class="line">  optional uint32 local_size = 13 [default = 5]; // for local response norm</div><div class="line">  optional float alpha = 14 [default = 1.]; // for local response norm</div><div class="line">  optional float beta = 15 [default = 0.75]; // for local response norm</div><div class="line">  optional float k = 22 [default = 1.];</div><div class="line"></div><div class="line">  // For data layers, specify the data source</div><div class="line">  optional string source = 16;</div><div class="line">  // For data pre-processing, we can do simple scaling and subtracting the</div><div class="line">  // data mean, if provided. Note that the mean subtraction is always carried</div><div class="line">  // out before scaling.</div><div class="line">  optional float scale = 17 [default = 1];</div><div class="line">  optional string meanfile = 18;</div><div class="line">  // For data layers, specify the batch size.</div><div class="line">  optional uint32 batchsize = 19;</div><div class="line">  // For data layers, specify if we would like to randomly crop an image.</div><div class="line">  optional uint32 cropsize = 20 [default = 0];</div><div class="line">  // For data layers, specify if we want to randomly mirror data.</div><div class="line">  optional bool mirror = 21 [default = false];</div><div class="line"></div><div class="line">  // The blobs containing the numeric parameters of the layer</div><div class="line">  repeated BlobProto blobs = 50;</div><div class="line">  // The ratio that is multiplied on the global learning rate. If you want to</div><div class="line">  // set the learning ratio for one blob, you need to set it for all blobs.</div><div class="line">  repeated float blobs_lr = 51;</div><div class="line">  // The weight decay that is multiplied on the global weight decay.</div><div class="line">  repeated float weight_decay = 52;</div><div class="line"></div><div class="line">  // The rand_skip variable is for the data layer to skip a few data points</div><div class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</div><div class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</div><div class="line">  // be larger than the number of keys in the database.</div><div class="line">  optional uint32 rand_skip = 53 [default = 0];</div><div class="line"></div><div class="line">  // Fields related to detection (det_*)</div><div class="line">  // foreground (object) overlap threshold</div><div class="line">  optional float det_fg_threshold = 54 [default = 0.5];</div><div class="line">  // background (non-object) overlap threshold</div><div class="line">  optional float det_bg_threshold = 55 [default = 0.5];</div><div class="line">  // Fraction of batch that should be foreground objects</div><div class="line">  optional float det_fg_fraction = 56 [default = 0.25];</div><div class="line"></div><div class="line">  // optional bool OBSOLETE_can_clobber = 57 [default = true];</div><div class="line"></div><div class="line">  // Amount of contextual padding to add around a window</div><div class="line">  // (used only by the window_data_layer)</div><div class="line">  optional uint32 det_context_pad = 58 [default = 0];</div><div class="line"></div><div class="line">  // Mode for cropping out a detection window</div><div class="line">  // warp: cropped window is warped to a fixed size and aspect ratio</div><div class="line">  // square: the tightest square around the window is cropped</div><div class="line">  optional string det_crop_mode = 59 [default = "warp"];</div><div class="line"></div><div class="line">  // For ReshapeLayer, one needs to specify the new dimensions.</div><div class="line">  optional int32 new_num = 60 [default = 0];</div><div class="line">  optional int32 new_channels = 61 [default = 0];</div><div class="line">  optional int32 new_height = 62 [default = 0];</div><div class="line">  optional int32 new_width = 63 [default = 0];</div><div class="line"></div><div class="line">  // Whether or not ImageLayer should shuffle the list of files at every epoch.</div><div class="line">  // It will also resize images if new_height or new_width are not zero.</div><div class="line">  optional bool shuffle_images = 64 [default = false];</div><div class="line"></div><div class="line">  // For ConcatLayer, one needs to specify the dimension for concatenation, and</div><div class="line">  // the other dimensions must be the same for all the bottom blobs.</div><div class="line">  // By default it will concatenate blobs along the channels dimension.</div><div class="line">  optional uint32 concat_dim = 65 [default = 1];</div><div class="line"></div><div class="line">  optional HDF5OutputParameter hdf5_output_param = 1001;</div><div class="line">&#125;</div><div class="line">//PReLUParameter,源于论文</div><div class="line">message PReLUParameter &#123;</div><div class="line">  // Parametric ReLU described in K. He et al, Delving Deep into Rectifiers:</div><div class="line">  // Surpassing Human-Level Performance on ImageNet Classification, 2015.</div><div class="line"></div><div class="line">  // Initial value of a_i. Default is a_i=0.25 for all i.</div><div class="line">  optional FillerParameter filler = 1;</div><div class="line">  // Whether or not slope paramters are shared across channels.</div><div class="line">  optional bool channel_shared = 2 [default = false];</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/03/07/Batch-Normalization/" rel="next" title="Batch Normalization">
                <i class="fa fa-chevron-left"></i> Batch Normalization
              </a>
            
			
			
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <div class="ds-share flat" data-thread-key="2017/03/10/Protobuf-and-caffe-proto/"
     data-title="Protobuf and caffe.proto"
     data-content=""
     data-url="http://verdin.cn/2017/03/10/Protobuf-and-caffe-proto/">
  <div class="ds-share-inline">
    <ul  class="ds-share-icons-16">

      <li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li>
      <li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li>
      <li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li>
      <li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li>
      <li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li>

    </ul>
    <div class="ds-share-icons-more">
    </div>
  </div>
</div>
      
    </div>
  </div>

          
          </div>
          


          
<section id="comment">  
  <!-- 多说评论框 start -->
	<div class="ds-thread" data-thread-key="<%- page.path %>" data-title="<%- page.title %>" data-url="<%- page.permalink %>"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"verdin"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->
</section>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Verdin" />
          <p class="site-author-name" itemprop="name">Verdin</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么要使用protobuf？"><span class="nav-number">1.</span> <span class="nav-text">为什么要使用protobuf？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#训练过程"><span class="nav-number">1.1.</span> <span class="nav-text">训练过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#protobuf的使用流程"><span class="nav-number">2.</span> <span class="nav-text">protobuf的使用流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#简易caffe-proto编写解析示例"><span class="nav-number">3.</span> <span class="nav-text">简易caffe.proto编写解析示例</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Verdin</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

<span id="busuanzi_container_site_pv">
  . . . . . . . . . 访问量<span id="busuanzi_value_site_pv"></span>次
</span>

<span id="busuanzi_container_site_uv">
  _______~欢迎第<span id="busuanzi_value_site_uv"></span>位访客~
</span>



        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"verdin"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  













  
  

  

  

  

  


  

</body>
</html>
